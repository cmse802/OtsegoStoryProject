{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uzairname/OtsegoStoryProject/blob/main/experiments/Final_Analysis_Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to view the data, it is available at https://docs.google.com/spreadsheets/d/1WMoeCGweQA0xUtEwlK8R_XzRuZOgPqpHQDGWizb9slQ/"
      ],
      "metadata": {
        "id": "sX97uNt4ZBKi"
      },
      "id": "sX97uNt4ZBKi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to use Gemma, follow the instructions that show up after running the cell below to create a hugging face API key, and enter it when prompted.\n",
        "\n",
        "If asked to add the token as a git credential, type n."
      ],
      "metadata": {
        "id": "SDRSfp_RMlqU"
      },
      "id": "SDRSfp_RMlqU"
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login"
      ],
      "metadata": {
        "id": "ekspuZ_-MMQJ"
      },
      "id": "ekspuZ_-MMQJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bertopic bitsandbytes accelerate -q"
      ],
      "metadata": {
        "id": "sqo6lfLvrrOh"
      },
      "id": "sqo6lfLvrrOh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "ZBTbYBOo9cv7"
      },
      "id": "ZBTbYBOo9cv7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaca5acc",
      "metadata": {
        "id": "aaca5acc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "from datetime import timedelta\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Data Loading & Preprocessing\n",
        "# -------------------------------\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load CSV\n",
        "csv_url = f'https://docs.google.com/spreadsheets/d/1WMoeCGweQA0xUtEwlK8R_XzRuZOgPqpHQDGWizb9slQ/export?format=csv'\n",
        "df = pd.read_csv(csv_url)\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "df = df[df['timestamp'].notnull()]  # Remove rows with invalid timestamps\n",
        "df['content'] = df['content'].fillna(\"\").astype(str)\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # Replace newlines with a period and a space\n",
        "    text = re.sub(r'[\\r\\n]+', '. ', text)\n",
        "\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Filter out short sentences\n",
        "    filtered_sentences = [s for s in sentences if len(s) >= 20]\n",
        "\n",
        "    # Remove unwanted characters but preserve punctuation: , \" . ? !\n",
        "    cleaned_sentences = [\n",
        "        re.sub(r'[^a-zA-Z0-9\\s,.!?\\\"\\'â€™]', '', s)\n",
        "        for s in filtered_sentences\n",
        "    ]\n",
        "\n",
        "    # Optionally: lowercase everything for consistency\n",
        "    cleaned_sentences = [s.lower() for s in cleaned_sentences]\n",
        "\n",
        "    return ' '.join(cleaned_sentences)\n",
        "\n",
        "# Remove \"See more\"\n",
        "df['content'] = df['content'].str.replace(\"\\nSee more\", \"\", regex=False)\n",
        "\n",
        "# Apply the preprocessing function to the content column\n",
        "df['cleaned_content'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "# remove empty rows\n",
        "df = df[df['cleaned_content'] != '']\n",
        "\n",
        "# Inspect the first few rows of the new column\n",
        "print(df[['content', 'cleaned_content']].head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df['content'].str.len() < 21)]"
      ],
      "metadata": {
        "id": "pY7xyu-HOvj2"
      },
      "id": "pY7xyu-HOvj2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Sentiment Analysis\n",
        "\n",
        "The following cell computes sentiment scores via BERT and Vader. It may take a few minutes"
      ],
      "metadata": {
        "id": "Fy_D_XIUJq8Q"
      },
      "id": "Fy_D_XIUJq8Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55016140",
      "metadata": {
        "id": "55016140"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# 2. Sentiment Analysis: VADER\n",
        "# -------------------------------\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "df['vader_compound'] = df['content'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Sentiment Analysis: BERT\n",
        "# -------------------------------\n",
        "# Initialize a BERT sentiment-analysis pipeline.\n",
        "\n",
        "\n",
        "bert_pipeline = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "def get_bert_compound(text):\n",
        "    # Get the sentiment result (returns a list of dicts)\n",
        "    result = bert_pipeline(text)\n",
        "    # Extract the star rating from the label (e.g., \"4 stars\")\n",
        "    label = result[0]['label']\n",
        "    rating = int(label.split()[0])\n",
        "    # Map rating (1-5) to a compound score between -1 and 1:\n",
        "    # 1 -> -1.0, 2 -> -0.5, 3 -> 0.0, 4 -> 0.5, 5 -> 1.0\n",
        "    compound = (rating - 3) / 2.0\n",
        "    return compound\n",
        "\n",
        "# Apply BERT sentiment analysis (this may take a bit, depending on the dataset size)\n",
        "# df['bert_compound'] = df['content'].progress_apply(get_bert_compound)\n",
        "\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "df['sentiment'] = df['content'].progress_apply(lambda x: sentiment_pipeline(x)[0]['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit Topic Model\n",
        "\n",
        "The following cells train a topic model on the posts. It assigns a topic index to each post. We can then analyze the posts by topic, date, and sentiment."
      ],
      "metadata": {
        "id": "7WVSI3ypJbF4"
      },
      "id": "7WVSI3ypJbF4"
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import HDBSCAN\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from umap import UMAP\n",
        "\n",
        "model = BERTopic(\n",
        "    embedding_model=\"all-MiniLM-L6-v2\",\n",
        "    top_n_words=10,\n",
        "    min_topic_size=10,\n",
        "    n_gram_range=(1, 2),\n",
        "    vectorizer_model=CountVectorizer(ngram_range=(1, 2), stop_words=\"english\"),\n",
        "    representation_model=KeyBERTInspired(),\n",
        "    hdbscan_model= HDBSCAN(min_cluster_size=20),\n",
        "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine'),\n",
        "    verbose=True,\n",
        "    nr_topics=6,\n",
        ")\n",
        "\n",
        "topics, probs = model.fit_transform(df['cleaned_content'])\n",
        "df['topic'] = topics\n"
      ],
      "metadata": {
        "id": "cogGvM3grprg"
      },
      "id": "cogGvM3grprg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Topic Names\n",
        "\n",
        "The following cells will use a language model to assign the topics short titles in natural language. This is a free, open source model from hugging face. It may take a minute to download the model."
      ],
      "metadata": {
        "id": "1I8ctV2AJETk"
      },
      "id": "1I8ctV2AJETk"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# client = OpenAI()\n",
        "# def chat(prompt):\n",
        "#   response = client.responses.create(\n",
        "#       model=\"gpt-4o-mini\",\n",
        "#       input=prompt\n",
        "#   )\n",
        "#.  return response.output_text\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\")"
      ],
      "metadata": {
        "id": "KZj9S2LaCBMr"
      },
      "id": "KZj9S2LaCBMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(prompt):\n",
        "  from transformers import pipeline\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "  output = pipe(messages)\n",
        "  return output[0]['generated_text'][-1]['content']\n",
        "\n",
        "def get_topic_info(model, topic_id, df, n_docs=10):\n",
        "  topic_docs = df[df['topic']==topic_id]\n",
        "  topic_docs = topic_docs.sample(min(n_docs, len(topic_docs)))\n",
        "  keywords = ', '.join([i for i in model.generate_topic_labels(nr_words=5) if int(i.split('_')[0]) == topic_id][0].split('_')[1:])\n",
        "  return topic_docs['content'].tolist(), keywords\n"
      ],
      "metadata": {
        "id": "jMhmmwVFJ8Bk"
      },
      "id": "jMhmmwVFJ8Bk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "topic_names = {}\n",
        "for topic_id in list(set(model.topics_)):\n",
        "  docs, keywords = get_topic_info(model, topic_id, df)\n",
        "\n",
        "  docs_str = \"\\n\".join(docs)\n",
        "\n",
        "  prompt = docs_str + f\"\\n\\nYou are an AI topic model. Above are a subset of facebook posts from a community environmental health advocacy group in Otsego, MI that follow a common theme. The theme has the key words \\\"{keywords}\\\". Please come up with a clear and accurate name representing the subject of what people are discussing in 5 words or less. Respond with just the name and nothing else.\"\n",
        "\n",
        "  print(prompt)\n",
        "\n",
        "  response = chat(prompt)\n",
        "  print(response)\n",
        "\n",
        "  topic_names[topic_id]=response.strip()\n",
        "\n",
        "df['topic_name'] = df['topic'].map(topic_names)\n"
      ],
      "metadata": {
        "id": "KV4Y_8rHuD4W"
      },
      "id": "KV4Y_8rHuD4W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JvXLs0YAILPp"
      },
      "id": "JvXLs0YAILPp"
    },
    {
      "cell_type": "code",
      "source": [
        "df['topic_name'].value_counts()"
      ],
      "metadata": {
        "id": "oImYJyYfvZhb"
      },
      "id": "oImYJyYfvZhb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Figures"
      ],
      "metadata": {
        "id": "YNk5SN9jPM6J"
      },
      "id": "YNk5SN9jPM6J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment by topic"
      ],
      "metadata": {
        "id": "5XD3xuD_POpQ"
      },
      "id": "5XD3xuD_POpQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Determine mean sentiment by topics\n",
        "df.groupby('topic_name').agg({'vader_compound': 'mean'})\n",
        "\n",
        "# Determine mean sentiment by topics over time\n",
        "sentiment_by_topic_year = df.groupby(['topic_name', df['timestamp'].dt.year]).agg({'vader_compound': 'mean', 'bert_compound': 'mean'})\n",
        "\n",
        "sentiment_by_topic_year = sentiment_by_topic_year.reset_index()\n",
        "\n",
        "# Create the Plotly line plot\n",
        "fig = px.line(\n",
        "    sentiment_by_topic_year,\n",
        "    x='timestamp',\n",
        "    y=['vader_compound'],\n",
        "    color='topic_name',\n",
        "    title='Mean Sentiment by Topic Over Time',\n",
        "    labels={'timestamp': 'Year', 'value': 'Mean Sentiment', 'topic_name': 'Topic'},\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "7iuxmB_6Dyca"
      },
      "id": "7iuxmB_6Dyca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XuDpkvzPUVZ"
      },
      "id": "2XuDpkvzPUVZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 4. Daily Aggregation & Forecasting\n",
        "# -------------------------------\n",
        "# Create a date column\n",
        "df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "# Aggregate daily average sentiment for VADER and BERT\n",
        "daily_vader = df.groupby('date')['vader_compound'].mean().reset_index().rename(columns={'vader_compound': 'avg_compound'})\n",
        "daily_bert = df.groupby('date')['bert_compound'].mean().reset_index().rename(columns={'bert_compound': 'avg_compound'})\n",
        "df['day_mean_vader'] = df.groupby('date')['vader_compound'].transform('mean')\n",
        "df['day_mean_bert'] = df.groupby('date')['bert_compound'].transform('mean')\n",
        "\n",
        "\n",
        "# Forecast function using linear regression\n",
        "def forecast_sentiment():\n",
        "    # Convert dates to ordinal numbers for regression\n",
        "    daily_df['date_ordinal'] = pd.to_datetime(daily_df['date']).apply(lambda date: date.toordinal())\n",
        "    X = daily_df['date_ordinal'].values.reshape(-1, 1)\n",
        "    y = daily_df['avg_compound'].values\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Forecast from the day after the last date until the end of 2026\n",
        "    last_date = pd.to_datetime(daily_df['date'].max())\n",
        "    future_dates = pd.date_range(start=last_date + timedelta(days=1), end=pd.Timestamp(\"2026-12-31\"))\n",
        "    future_ordinals = np.array([d.toordinal() for d in future_dates]).reshape(-1, 1)\n",
        "    predicted = model.predict(future_ordinals)\n",
        "    future_df = pd.DataFrame({'date': future_dates, 'predicted_compound': predicted})\n",
        "    return future_df\n",
        "\n",
        "vader_forecast = forecast_sentiment(df[['date', 'day_mean_vader']].rename(columns={'day_mean_vader': 'avg_compound'}))\n",
        "bert_forecast = forecast_sentiment(df[['date', 'day_mean_bert']].rename(columns={'day_mean_bert': 'avg_compound'}))\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Visualization\n",
        "# -------------------------------\n",
        "\n",
        "# Figure 1: Trend Plots (side-by-side) for VADER and BERT with Forecasts\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "# VADER Plot\n",
        "axs[0].plot(daily_vader['date'], df['avg_compound'], marker='o', label='Historical')\n",
        "axs[0].plot(vader_forecast['date'], vader_forecast['predicted_compound'], marker='x', linestyle='--', label='Forecast')\n",
        "axs[0].set_xlabel('Date', fontsize=12)\n",
        "axs[0].set_ylabel('Avg Compound Sentiment', fontsize=12)\n",
        "axs[0].set_title('VADER Sentiment Trend & Forecast', fontsize=14)\n",
        "axs[0].set_ylim(-1, 1)\n",
        "axs[0].axhline(y=0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[0].axhline(y=-0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[0].grid(True)\n",
        "axs[0].legend(fontsize=10)\n",
        "axs[0].text(daily_vader['date'].iloc[-1], 0.07, 'Positive', color='green', fontsize=10)\n",
        "axs[0].text(daily_vader['date'].iloc[-1], 0.00, 'Neutral', color='blue', fontsize=10)\n",
        "axs[0].text(daily_vader['date'].iloc[-1], -0.09, 'Negative', color='red', fontsize=10)\n",
        "\n",
        "# BERT Plot\n",
        "axs[1].plot(daily_bert['date'], daily_bert['avg_compound'], marker='o', label='Historical')\n",
        "axs[1].plot(bert_forecast['date'], bert_forecast['predicted_compound'], marker='x', linestyle='--', label='Forecast')\n",
        "axs[1].set_xlabel('Date', fontsize=12)\n",
        "axs[1].set_title('BERT Sentiment Trend & Forecast', fontsize=14)\n",
        "axs[1].set_ylim(-1, 1)\n",
        "axs[1].axhline(y=0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[1].axhline(y=-0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[1].grid(True)\n",
        "axs[1].legend(fontsize=10)\n",
        "axs[1].text(daily_bert['date'].iloc[-1], 0.07, 'Positive', color='green', fontsize=10)\n",
        "axs[1].text(daily_bert['date'].iloc[-1], 0.00, 'Neutral', color='blue', fontsize=10)\n",
        "axs[1].text(daily_bert['date'].iloc[-1], -0.09, 'Negative', color='red', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Figure 2: Scatter Plot Comparing VADER vs. BERT for Each Post\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['vader_compound'], df['bert_compound'], alpha=0.6)\n",
        "plt.xlabel('VADER Compound Score', fontsize=12)\n",
        "plt.ylabel('BERT Compound Score', fontsize=12)\n",
        "plt.title('VADER vs. BERT Sentiment Scores', fontsize=14)\n",
        "plt.grid(True)\n",
        "lims = [-1, 1]\n",
        "plt.plot(lims, lims, 'r--', linewidth=1)  # Diagonal reference line\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Figure 3: Histograms of Sentiment Distributions for VADER and BERT\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
        "\n",
        "ax1.hist(df['vader_compound'], bins=20, color='skyblue', edgecolor='black')\n",
        "ax1.set_title('VADER Sentiment Distribution', fontsize=14)\n",
        "ax1.set_xlabel('VADER Compound Score', fontsize=12)\n",
        "ax1.set_ylabel('Frequency', fontsize=12)\n",
        "ax1.set_xlim(-1, 1)\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.hist(df['bert_compound'], bins=20, color='salmon', edgecolor='black')\n",
        "ax2.set_title('BERT Sentiment Distribution', fontsize=14)\n",
        "ax2.set_xlabel('BERT Compound Score', fontsize=12)\n",
        "ax2.set_xlim(-1, 1)\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Summary Statistics & Analysis Output\n",
        "# -------------------------------"
      ],
      "metadata": {
        "id": "RRZlokpIzXJF"
      },
      "id": "RRZlokpIzXJF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf3ae7e",
      "metadata": {
        "id": "0cf3ae7e"
      },
      "outputs": [],
      "source": [
        "# Additional Visualizations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Visualization 4: Box Plots of VADER and BERT Compound Scores by Day of Week ---\n",
        "\n",
        "# Create a 'day_of_week' column in the DataFrame with proper ordering\n",
        "days_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "df['day_of_week'] = pd.Categorical(df['timestamp'].dt.day_name(), categories=days_order, ordered=True)\n",
        "\n",
        "# Prepare the data lists for each day for both VADER and BERT scores\n",
        "vader_box_data = [df.loc[df['day_of_week'] == day, 'vader_compound'].dropna() for day in days_order]\n",
        "bert_box_data = [df.loc[df['day_of_week'] == day, 'bert_compound'].dropna() for day in days_order]\n",
        "\n",
        "# Create side-by-side box plots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "axs[0].boxplot(vader_box_data, labels=days_order)\n",
        "axs[0].set_title(\"VADER Compound Scores by Day of Week\")\n",
        "axs[0].set_xlabel(\"Day of Week\")\n",
        "axs[0].set_ylabel(\"Compound Score\")\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].boxplot(bert_box_data, labels=days_order)\n",
        "axs[1].set_title(\"BERT Compound Scores by Day of Week\")\n",
        "axs[1].set_xlabel(\"Day of Week\")\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Visualization 5: Stacked Bar Chart of VADER Sentiment Categories by Day of Week ---\n",
        "\n",
        "# Define a helper function to assign sentiment categories for VADER\n",
        "def categorize(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Apply the function to create a new category column for VADER scores\n",
        "df['vader_category'] = df['vader_compound'].apply(categorize)\n",
        "\n",
        "# Create a pivot table: counts of sentiment categories by day of week\n",
        "pivot = df.groupby('day_of_week')['vader_category'].value_counts().unstack().fillna(0).loc[days_order]\n",
        "\n",
        "# Plot a stacked bar chart for the VADER sentiment categories by day\n",
        "pivot.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
        "plt.title(\"VADER Sentiment Categories by Day of Week\")\n",
        "plt.xlabel(\"Day of Week\")\n",
        "plt.ylabel(\"Number of Posts\")\n",
        "plt.legend(title=\"Sentiment Category\")\n",
        "plt.grid(True, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Summary Statistics:\")\n",
        "\n",
        "total_posts = len(df)\n",
        "print(f\"Total posts analyzed: {total_posts}\")\n",
        "\n",
        "avg_vader = df['vader_compound'].mean()\n",
        "avg_bert = df['bert_compound'].mean()\n",
        "std_vader = df['vader_compound'].std()\n",
        "std_bert = df['bert_compound'].std()\n",
        "\n",
        "print(f\"Average VADER compound score: {avg_vader:.3f}\")\n",
        "print(f\"Average BERT compound score: {avg_bert:.3f}\")\n",
        "print(f\"Standard Deviation VADER compound score: {std_vader:.3f}\")\n",
        "print(f\"Standard Deviation BERT compound score: {std_bert:.3f}\")\n",
        "\n",
        "# Define a helper function to assign a sentiment category\n",
        "def categorize(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "vader_categories = df['vader_compound'].apply(categorize)\n",
        "bert_categories = df['bert_compound'].apply(categorize)\n",
        "\n",
        "vader_counts = vader_categories.value_counts()\n",
        "bert_counts = bert_categories.value_counts()\n",
        "\n",
        "print(\"\\nVADER Sentiment Category Counts:\")\n",
        "print(vader_counts)\n",
        "print(\"\\nBERT Sentiment Category Counts:\")\n",
        "print(bert_counts)\n",
        "\n",
        "corr = df['vader_compound'].corr(df['bert_compound'])\n",
        "print(f\"\\nCorrelation between VADER and BERT compound scores: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "685ddc30",
      "metadata": {
        "id": "685ddc30"
      },
      "source": [
        "# Sentiment Analysis Summary\n",
        "\n",
        "**Total Posts Analyzed:** 256\n",
        "\n",
        "## Overall Sentiment\n",
        "- **VADER Average Compound:** 0.086  \n",
        "- **BERT Average Compound:** 0.092  \n",
        "*Both methods show slightly positive averages, but these can mask underlying complexity.*\n",
        "\n",
        "- **VADER Std. Dev.:** 0.449  \n",
        "- **BERT Std. Dev.:** 0.800  \n",
        "*BERT exhibits a wider range, indicating more extreme sentiment values.*\n",
        "\n",
        "## Sentiment Categories\n",
        "- **VADER Counts:**\n",
        "  - Positive: 110\n",
        "  - Neutral: 84\n",
        "  - Negative: 62\n",
        "\n",
        "- **BERT Counts:**\n",
        "  - Positive: 117\n",
        "  - Negative: 91\n",
        "  - Neutral: 48\n",
        "\n",
        "*BERT flags more negative posts, suggesting higher sensitivity to distress.*\n",
        "\n",
        "## Correlation Between Methods\n",
        "- **Correlation:** 0.437  \n",
        "*Moderate correlation indicates the two methods capture sentiment differently.*\n",
        "\n",
        "## Insights & Recommendations\n",
        "- **Context Matters:**  \n",
        "  The slightly positive averages hide variability. BERTâ€™s higher negative count may better reflect community distress.\n",
        "  \n",
        "- **Multiple Methods:**  \n",
        "  Using both VADER and BERT provides a fuller picture. Consider fine-tuning BERT on domain-specific data for improved accuracy.\n",
        "\n",
        "- **Additional Visualizations:**  \n",
        "  - **Heatmaps/Calendar Plots:** Identify periods with heightened negative sentiment.  \n",
        "  - **Topic Modeling:** Use LDA to link themes (e.g., water quality, cancer) with sentiment trends.  \n",
        "  - **Event Annotations:** Overlay key community events on trend graphs to contextualize shifts in sentiment.\n",
        "\n",
        "## Conclusion\n",
        "While overall sentiment appears slightly positive, the variability and methodological differences (especially BERTâ€™s detection of more negative sentiment) suggest that the community's emotional response is more nuanced and polarized. This underscores the need for a combined quantitative and qualitative approach to fully capture the community's experiences, particularly given the serious issues of water quality and health.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca902eb",
      "metadata": {
        "id": "5ca902eb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}