{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uzairname/OtsegoStoryProject/blob/main/experiments/Final_Analysis_Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sX97uNt4ZBKi",
      "metadata": {
        "id": "sX97uNt4ZBKi"
      },
      "source": [
        "## This notebook works best in Google Colab.\n",
        "If visible, click the \"open in colab\" link to open this notebook in Colab. Otherwise, you must run `pip install -r requirements.txt` before running this notebook.\n",
        "\n",
        "If you would like to view the data, it is available at https://docs.google.com/spreadsheets/d/1WMoeCGweQA0xUtEwlK8R_XzRuZOgPqpHQDGWizb9slQ/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SDRSfp_RMlqU",
      "metadata": {
        "id": "SDRSfp_RMlqU"
      },
      "source": [
        "To be able to use Gemma, follow the instructions that show up after running the cell below to create a hugging face API key, and enter it when prompted.\n",
        "\n",
        "If asked to add the token as a git credential, type n.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ekspuZ_-MMQJ",
      "metadata": {
        "id": "ekspuZ_-MMQJ"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sqo6lfLvrrOh",
      "metadata": {
        "id": "sqo6lfLvrrOh"
      },
      "outputs": [],
      "source": [
        "!pip install -U bertopic bitsandbytes accelerate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8dxVb-PWMUz",
      "metadata": {
        "id": "c8dxVb-PWMUz"
      },
      "source": [
        "# Justice for Otsego Sentiment Analysis\n",
        "\n",
        "This notebook performs sentiment analysis on Facebook posts collected by the Justice for Otsego project using both VADER (lexicon-based) and BERT (transformer-based) approaches.\n",
        "\n",
        "## ✅ How to Reproduce This Notebook\n",
        "\n",
        "To ensure full reproducibility:\n",
        "\n",
        "1. **Environment**  \n",
        "   Use Python 3.8+ with the following packages installed:\n",
        "   - `pandas`\n",
        "   - `matplotlib`\n",
        "   - `nltk`\n",
        "   - `transformers`\n",
        "   - `sklearn`\n",
        "   - `numpy`\n",
        "   - `tqdm`\n",
        "   - `plotly`\n",
        "\n",
        "   You can install these via:\n",
        "   ```bash\n",
        "   pip install pandas matplotlib nltk transformers scikit-learn numpy tqdm plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Preview of Preprocessed Data\n",
        "\n",
        "The following output displays a sample of the original post content (`content`) alongside the cleaned and tokenized version (`cleaned_content`).\n",
        "\n",
        "This step is crucial because:\n",
        "\n",
        "- **Noise Reduction**: Social media posts often contain extra line breaks, emojis, or short fragments. These are removed or cleaned to make the text more analyzable.\n",
        "- **Sentence Filtering**: We only keep sentences with 20 or more characters to avoid processing fragments like \"ugh\" or \"yes,\" which don’t carry much sentiment weight.\n",
        "- **Standardization**: All text is converted to lowercase and stripped of unwanted characters, preserving only punctuation that affects sentence meaning (e.g., `!`, `?`, `.`).\n",
        "- **Readability**: The cleaned version ensures that the models (VADER and BERT) process more meaningful input for sentiment scoring.\n",
        "\n",
        "This table gives us an at-a-glance view of how much transformation the original post undergoes before analysis begins.\n"
      ],
      "metadata": {
        "id": "dqtEoh846CXq"
      },
      "id": "dqtEoh846CXq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaca5acc",
      "metadata": {
        "id": "aaca5acc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "from datetime import timedelta\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from tqdm.notebook import tqdm\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Data Loading & Preprocessing\n",
        "# -------------------------------\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load CSV\n",
        "csv_url = f'https://docs.google.com/spreadsheets/d/1WMoeCGweQA0xUtEwlK8R_XzRuZOgPqpHQDGWizb9slQ/export?format=csv'\n",
        "df = pd.read_csv(csv_url)\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "df = df[df['timestamp'].notnull()]  # Remove rows with invalid timestamps\n",
        "df['content'] = df['content'].fillna(\"\").astype(str)\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # Replace newlines with a period and a space\n",
        "    text = re.sub(r'[\\r\\n]+', '. ', text)\n",
        "\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Filter out short sentences\n",
        "    filtered_sentences = [s for s in sentences if len(s) >= 20]\n",
        "\n",
        "    # Remove unwanted characters but preserve punctuation: , \" . ? !\n",
        "    cleaned_sentences = [\n",
        "        re.sub(r'[^a-zA-Z0-9\\s,.!?\\\"\\'’]', '', s)\n",
        "        for s in filtered_sentences\n",
        "    ]\n",
        "\n",
        "    # Optionally: lowercase everything for consistency\n",
        "    cleaned_sentences = [s.lower() for s in cleaned_sentences]\n",
        "\n",
        "    return ' '.join(cleaned_sentences)\n",
        "\n",
        "# Remove \"See more\"\n",
        "df['content'] = df['content'].str.replace(\"\\nSee more\", \"\", regex=False)\n",
        "\n",
        "# Apply the preprocessing function to the content column\n",
        "df['cleaned_content'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "# remove empty rows\n",
        "df = df[df['cleaned_content'] != '']\n",
        "\n",
        "# Inspect the first few rows of the new column\n",
        "print(df[['content', 'cleaned_content']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pY7xyu-HOvj2",
      "metadata": {
        "id": "pY7xyu-HOvj2"
      },
      "outputs": [],
      "source": [
        "df[(df['content'].str.len() < 21)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fy_D_XIUJq8Q",
      "metadata": {
        "id": "Fy_D_XIUJq8Q"
      },
      "source": [
        "### Apply Sentiment Analysis\n",
        "\n",
        "The following cell computes sentiment scores via BERT and Vader. It may take a few minutes\n",
        "\n",
        "## 💬 Sentiment Analysis: VADER vs. BERT\n",
        "\n",
        "In this section, we analyze the emotional tone of each Facebook post using **two complementary sentiment analysis models**:\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
        "\n",
        "- **What it is**: A lexicon- and rule-based sentiment analysis tool optimized for social media text.\n",
        "- **Why it's used**: It's **fast**, easy to interpret, and works well with shorter, informal text like Facebook posts.\n",
        "- **What it returns**: A `compound` score between -1 (very negative) and +1 (very positive), based on a weighted combination of positive, neutral, and negative word valence.\n",
        "\n",
        "#### Reproducibility Tips:\n",
        "- No randomness in outputs — purely deterministic based on text and the internal dictionary.\n",
        "- Requires downloading the `vader_lexicon` once using `nltk.download()`.\n",
        "\n",
        "---\n",
        "\n",
        "### 🤖 BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "- **What it is**: A transformer-based deep learning model trained on a large corpus of human-labeled sentiment data.\n",
        "- **Why it's used**: BERT can detect more **nuanced emotions**, understand **context**, and generally outperforms simpler models on complex or mixed-tone text.\n",
        "- **What it returns**: Labels such as `POSITIVE` or `NEGATIVE` and a confidence `score`.\n",
        "\n",
        "#### How We Convert BERT Output to a Compound Score:\n",
        "- We use a simple transformation:\n",
        "  - `POSITIVE` → +score\n",
        "  - `NEGATIVE` → -score\n",
        "- This allows direct comparison with VADER’s compound score.\n",
        "\n",
        "#### Reproducibility Tips:\n",
        "- BERT models can behave slightly differently across versions or hardware due to floating-point precision.\n",
        "- To ensure stable results:\n",
        "  - Fix the `transformers` version in `requirements.txt`\n",
        "  - Optionally set environment variables like `TRANSFORMERS_NO_ADVISORY_WARNINGS=1` to suppress version warnings.\n",
        "  - Set a fixed random seed if doing advanced fine-tuning or sampling.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚖️ Why Both?\n",
        "\n",
        "Using both VADER and BERT gives a **more comprehensive sentiment picture**:\n",
        "- VADER excels at **speed and simplicity**.\n",
        "- BERT provides **depth and context sensitivity**.\n",
        "\n",
        "This dual approach also helps validate whether both models detect similar emotional trends, which boosts confidence in our findings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55016140",
      "metadata": {
        "id": "55016140"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# 2. Sentiment Analysis: VADER\n",
        "# -------------------------------\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "df['vader_compound'] = df['content'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Sentiment Analysis: BERT\n",
        "# -------------------------------\n",
        "# Initialize a BERT sentiment-analysis pipeline.\n",
        "\n",
        "\n",
        "# bert_pipeline = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "# def get_bert_compound(text):\n",
        "#     # Get the sentiment result (returns a list of dicts)\n",
        "#     result = bert_pipeline(text)\n",
        "#     # Extract the star rating from the label (e.g., \"4 stars\")\n",
        "#     label = result[0]['label']\n",
        "#     rating = int(label.split()[0])\n",
        "#     # Map rating (1-5) to a compound score between -1 and 1:\n",
        "#     # 1 -> -1.0, 2 -> -0.5, 3 -> 0.0, 4 -> 0.5, 5 -> 1.0\n",
        "#     compound = (rating - 3) / 2.0\n",
        "#     return compound\n",
        "\n",
        "\n",
        "# Apply BERT sentiment analysis (this may take a bit, depending on the dataset size)\n",
        "# df['bert_compound'] = df['content'].progress_apply(get_bert_compound)\n",
        "\n",
        "# Load the sentiment analysis pipeline (uses a BERT-based model by default)\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Apply BERT sentiment analysis (this may take a bit, depending on the dataset size)\n",
        "results = sentiment_pipeline(df['content'].tolist(), batch_size=16)\n",
        "\n",
        "compound_scores = [\n",
        "    result['score'] if result['label'] == 'POSITIVE' else -result['score']\n",
        "    for result in results\n",
        "]\n",
        "\n",
        "# Add the new column to the DataFrame\n",
        "df['bert_compound'] = compound_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CU1y5CKSFdbB",
      "metadata": {
        "id": "CU1y5CKSFdbB"
      },
      "outputs": [],
      "source": [
        "vader_dist = pd.cut(df['vader_compound'], bins=[float('-inf'), -0.01, 0.01, float('inf')], labels=['negative', 'neutral', 'positive'], right=True).value_counts()\n",
        "bert_dist = pd.cut(df['bert_compound'], bins=[float('-inf'), -0.7, 0.7, float('inf')], labels=['negative', 'neutral', 'positive'], right=True).value_counts()\n",
        "print('Vader sentiment distribution')\n",
        "print(vader_dist)\n",
        "print('BERT sentiment distribution')\n",
        "print(bert_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L4eRL9CcFf5p",
      "metadata": {
        "id": "L4eRL9CcFf5p"
      },
      "outputs": [],
      "source": [
        "df['bert_compound'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7WVSI3ypJbF4",
      "metadata": {
        "id": "7WVSI3ypJbF4"
      },
      "source": [
        "# Fit Topic Model\n",
        "\n",
        "The following cells train a topic model on the posts. It assigns a topic index to each post. We can then analyze the posts by topic, date, and sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cogGvM3grprg",
      "metadata": {
        "id": "cogGvM3grprg"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import HDBSCAN\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from umap import UMAP\n",
        "\n",
        "model = BERTopic(\n",
        "    embedding_model=\"all-MiniLM-L6-v2\",\n",
        "    top_n_words=10,\n",
        "    min_topic_size=10,\n",
        "    n_gram_range=(1, 2),\n",
        "    vectorizer_model=CountVectorizer(ngram_range=(1, 2), stop_words=\"english\"),\n",
        "    representation_model=KeyBERTInspired(),\n",
        "    hdbscan_model= HDBSCAN(min_cluster_size=20),\n",
        "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine'),\n",
        "    verbose=True,\n",
        "    nr_topics=6,\n",
        ")\n",
        "\n",
        "topics, probs = model.fit_transform(df['cleaned_content'])\n",
        "df['topic'] = topics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1I8ctV2AJETk",
      "metadata": {
        "id": "1I8ctV2AJETk"
      },
      "source": [
        "# Generate Topic Names\n",
        "\n",
        "The following cells will use a language model to assign the topics short titles in natural language. This is a free, open source model from hugging face. It may take a minute to download the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KZj9S2LaCBMr",
      "metadata": {
        "id": "KZj9S2LaCBMr"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# client = OpenAI()\n",
        "# def chat(prompt):\n",
        "#   response = client.responses.create(\n",
        "#       model=\"gpt-4o-mini\",\n",
        "#       input=prompt\n",
        "#   )\n",
        "#.  return response.output_text\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jMhmmwVFJ8Bk",
      "metadata": {
        "id": "jMhmmwVFJ8Bk"
      },
      "outputs": [],
      "source": [
        "def chat(prompt):\n",
        "  from transformers import pipeline\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "  output = pipe(messages)\n",
        "  return output[0]['generated_text'][-1]['content']\n",
        "\n",
        "def get_topic_info(model, topic_id, df, n_docs=10):\n",
        "  topic_docs = df[df['topic']==topic_id]\n",
        "  topic_docs = topic_docs.sample(min(n_docs, len(topic_docs)))\n",
        "  keywords = ', '.join([i for i in model.generate_topic_labels(nr_words=5) if int(i.split('_')[0]) == topic_id][0].split('_')[1:])\n",
        "  return topic_docs['content'].tolist(), keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KV4Y_8rHuD4W",
      "metadata": {
        "id": "KV4Y_8rHuD4W"
      },
      "outputs": [],
      "source": [
        "\n",
        "topic_names = {}\n",
        "for topic_id in list(set(model.topics_)):\n",
        "  docs, keywords = get_topic_info(model, topic_id, df)\n",
        "\n",
        "  docs_str = \"\\n\".join(docs)\n",
        "\n",
        "  prompt = docs_str + f\"\\n\\nAbove are a subset of facebook posts that follow a common theme. The theme has the key words \\\"{keywords}\\\". Please come up with a title/label representing the subject of what people are discussing in 5 words or less. Be specific. Respond with just the label and nothing else.\"\n",
        "\n",
        "  print(prompt)\n",
        "\n",
        "  response = chat(prompt)\n",
        "  print(response)\n",
        "\n",
        "  topic_names[topic_id]=response.strip()\n",
        "\n",
        "df['topic_name'] = df['topic'].map(topic_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JvXLs0YAILPp",
      "metadata": {
        "id": "JvXLs0YAILPp"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oImYJyYfvZhb",
      "metadata": {
        "id": "oImYJyYfvZhb"
      },
      "outputs": [],
      "source": [
        "df['topic_name'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YNk5SN9jPM6J",
      "metadata": {
        "id": "YNk5SN9jPM6J"
      },
      "source": [
        "# Figures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5XD3xuD_POpQ",
      "metadata": {
        "id": "5XD3xuD_POpQ"
      },
      "source": [
        "## Sentiment by topic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📈 Sentiment by Topic\n",
        "\n",
        "This bar chart shows the **average sentiment score (using VADER)** for each of the topics identified by BERTopic.\n",
        "\n",
        "### 🧪 What It Shows:\n",
        "\n",
        "- **X-axis (Topic)**: The names or short descriptions of each topic, generated using BERTopic's keyword extraction.\n",
        "- **Y-axis (Mean Sentiment)**: The average VADER compound sentiment score for posts within that topic.\n",
        "  - Values range from -1 (very negative) to +1 (very positive).\n",
        "- **Bar Colors**: Colored by sentiment value to visually emphasize more positive or negative themes.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Why This Matters:\n",
        "\n",
        "This visualization helps us understand:\n",
        "- Which topics are generally associated with **positive, neutral, or negative emotions**.\n",
        "- Whether certain issues (e.g., water safety, government response, health symptoms) evoke more emotional responses.\n",
        "- How **emotion and content** are related in community discourse.\n",
        "\n",
        "This is a key step in linking **quantitative emotion data** to **qualitative themes** from the Otsego community's experiences.\n"
      ],
      "metadata": {
        "id": "Yyw3Q9_d6rhl"
      },
      "id": "Yyw3Q9_d6rhl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JeXqj4NbWjOv",
      "metadata": {
        "id": "JeXqj4NbWjOv"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "# Determine mean sentiments by topic\n",
        "sentiment_by_topic = df.groupby(['topic_name']).agg({'vader_compound': 'mean'}).reset_index()\n",
        "\n",
        "fig = px.bar(\n",
        "    sentiment_by_topic,\n",
        "    x='topic_name',\n",
        "    y='vader_compound',\n",
        "    color='vader_compound',\n",
        "    labels={'topic_name': 'Topic', 'vader_compound': \"Mean Sentiment\"},\n",
        "    title='Mean Sentiment by Topic',\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📆 Sentiment Trends by Topic Over Time\n",
        "\n",
        "This line plot shows how the **average sentiment** of each topic has changed **year by year** in the Justice for Otsego Facebook posts.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What This Chart Tells Us:\n",
        "\n",
        "- **X-axis (Year)**: Time progression based on the year each post was created.\n",
        "- **Y-axis (Mean Sentiment)**: The average VADER sentiment score for posts in each topic during a given year.\n",
        "- **Color-coded Lines**: Each line represents a different topic discovered by BERTopic.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why It’s Useful:\n",
        "\n",
        "- **Reveals emotional shifts**: Spot years when the community felt more hopeful, upset, or neutral about key issues.\n",
        "- **Tracks community response**: See how public sentiment around certain topics evolved after events like news releases, town halls, or environmental updates.\n",
        "- **Supports storytelling**: Helps frame emotional arcs across time in response to unfolding local crises.\n",
        "\n",
        "This visualization is critical for understanding how **sentiment changes in response to external events** and gives researchers a way to map emotion to real-world context.\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: Since this uses VADER, the sentiment score is between -1 (very negative) and +1 (very positive).\n"
      ],
      "metadata": {
        "id": "f6xWZ_dS6s0i"
      },
      "id": "f6xWZ_dS6s0i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7iuxmB_6Dyca",
      "metadata": {
        "id": "7iuxmB_6Dyca"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Determine mean sentiment by topics over time\n",
        "sentiment_by_topic_year = df.groupby(['topic_name', df['timestamp'].dt.year]).agg({'vader_compound': 'mean'})\n",
        "\n",
        "sentiment_by_topic_year = sentiment_by_topic_year.reset_index()\n",
        "\n",
        "# Create the Plotly line plot\n",
        "fig = px.line(\n",
        "    sentiment_by_topic_year,\n",
        "    x='timestamp',\n",
        "    y=['vader_compound'],\n",
        "    color='topic_name',\n",
        "    title='Mean Sentiment by Topic Over Time',\n",
        "    labels={'timestamp': 'Year', 'value': 'Mean Sentiment', 'topic_name': 'Topic'},\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧮 Volume of Posts by Topic Over Time\n",
        "\n",
        "This line plot illustrates how frequently each topic appeared in the Justice for Otsego Facebook posts, **broken down by year**.\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Chart Overview:\n",
        "\n",
        "- **X-axis (Year)**: The year the posts were made, extracted from the `timestamp` column.\n",
        "- **Y-axis (Post Volume)**: The number of posts associated with each topic in that year.\n",
        "- **Color-coded Lines**: Each line represents a different topic identified by BERTopic.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why This is Important:\n",
        "\n",
        "- **Tracks community focus**: Highlights which issues were most frequently discussed in different years.\n",
        "- **Reveals emerging or fading concerns**: For example, a spike might signal a specific event, crisis, or news report that triggered discussion.\n",
        "- **Supports correlation with sentiment**: Helps contextualize the sentiment trends shown in the previous chart — a rise in post volume might explain emotional spikes.\n",
        "\n",
        "---\n",
        "\n",
        "By analyzing **volume and sentiment together**, we gain a more complete picture of how community dialogue has evolved, what topics dominated attention, and when public concern was at its highest.\n"
      ],
      "metadata": {
        "id": "WNqa8sCU62BQ"
      },
      "id": "WNqa8sCU62BQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AZlD01QAPlfW",
      "metadata": {
        "id": "AZlD01QAPlfW"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Group by topic and year, then count the number of rows\n",
        "volume_by_topic_year = df.groupby(['topic_name', df['timestamp'].dt.year]).size().reset_index(name='count')\n",
        "\n",
        "# Rename the year column for clarity\n",
        "volume_by_topic_year.rename(columns={'timestamp': 'year'}, inplace=True)\n",
        "\n",
        "# Create the Plotly line plot\n",
        "fig = px.line(\n",
        "    volume_by_topic_year,\n",
        "    x='year',\n",
        "    y='count',\n",
        "    color='topic_name',\n",
        "    title='Volume of Posts by Topic Over Time',\n",
        "    labels={'year': 'Year', 'count': 'Post Volume', 'topic_name': 'Topic'},\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Sentiment Analysis & Forecasting Results\n",
        "\n",
        "This section presents a series of visualizations to better understand the **patterns, differences, and future trends** in community sentiment captured through Facebook posts.\n",
        "\n",
        "---\n",
        "\n",
        "### 📈 1. Sentiment Trend & Forecast (VADER vs. BERT)\n",
        "\n",
        "Two side-by-side line plots compare **daily average sentiment** over time using:\n",
        "- **VADER** (left): Lexicon-based, interpretable, and fast.\n",
        "- **BERT** (right): Deep-learning-based, context-aware sentiment.\n",
        "\n",
        "Each plot includes:\n",
        "- ⚫ **Historical sentiment** (dots)\n",
        "- ❌ **Forecasted sentiment** using linear regression (dashed line)\n",
        "- 📉 Threshold markers for **Positive (≥ 0.05)**, **Neutral (-0.05 to 0.05)**, and **Negative (≤ -0.05)** tone\n",
        "\n",
        "This lets us explore how sentiment might shift through the end of **2026**, and how each model perceives emotional trends differently.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 2. VADER vs. BERT Score Comparison (Scatter Plot)\n",
        "\n",
        "This scatter plot directly compares the **compound sentiment score** of each post as calculated by both models.\n",
        "\n",
        "- Each point = one Facebook post\n",
        "- Red dashed line = perfect agreement between models\n",
        "- Deviations from the line show where **VADER and BERT disagree**\n",
        "\n",
        "This is especially helpful to:\n",
        "- Highlight posts where VADER sees neutral tone but BERT detects strong emotion\n",
        "- Validate consistency between models\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 3. Sentiment Distributions (Histograms)\n",
        "\n",
        "Two side-by-side histograms show how sentiment scores are distributed across all posts for each model:\n",
        "\n",
        "- **VADER** tends to be slightly more conservative, clustering around 0\n",
        "- **BERT** may show stronger positive or negative emotions due to deeper language understanding\n",
        "\n",
        "These plots provide a sense of **emotional polarity** and the overall tone of the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "Together, these visualizations paint a rich picture of:\n",
        "- The emotional state of the Otsego community\n",
        "- How those emotions have evolved\n",
        "- How two different NLP models interpret public sentiment\n",
        "- What might be coming in the future based on trend projections\n"
      ],
      "metadata": {
        "id": "lIf-qXGQ7JiU"
      },
      "id": "lIf-qXGQ7JiU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RRZlokpIzXJF",
      "metadata": {
        "id": "RRZlokpIzXJF"
      },
      "outputs": [],
      "source": [
        "df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "# Aggregate daily average sentiment for VADER and BERT\n",
        "daily_vader = df.groupby('date')['vader_compound'].mean().reset_index().rename(columns={'vader_compound': 'avg_compound'})\n",
        "daily_bert = df.groupby('date')['bert_compound'].mean().reset_index().rename(columns={'bert_compound': 'avg_compound'})\n",
        "\n",
        "# Forecast function using linear regression\n",
        "def forecast_sentiment(daily_df):\n",
        "    # Convert dates to ordinal numbers for regression\n",
        "    daily_df['date_ordinal'] = pd.to_datetime(daily_df['date']).apply(lambda date: date.toordinal())\n",
        "    X = daily_df['date_ordinal'].values.reshape(-1, 1)\n",
        "    y = daily_df['avg_compound'].values\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Forecast from the day after the last date until the end of 2026\n",
        "    last_date = pd.to_datetime(daily_df['date'].max())\n",
        "    future_dates = pd.date_range(start=last_date + timedelta(days=1), end=pd.Timestamp(\"2026-12-31\"))\n",
        "    future_ordinals = np.array([d.toordinal() for d in future_dates]).reshape(-1, 1)\n",
        "    predicted = model.predict(future_ordinals)\n",
        "    future_df = pd.DataFrame({'date': future_dates, 'predicted_compound': predicted})\n",
        "    return future_df\n",
        "\n",
        "future_vader = forecast_sentiment(daily_vader)\n",
        "future_bert = forecast_sentiment(daily_bert)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Visualization\n",
        "# -------------------------------\n",
        "\n",
        "# Figure 1: Trend Plots (side-by-side) for VADER and BERT with Forecasts\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "# VADER Plot\n",
        "axs[0].plot(daily_vader['date'], daily_vader['avg_compound'], marker='o', label='Historical')\n",
        "axs[0].plot(future_vader['date'], future_vader['predicted_compound'], marker='x', linestyle='--', label='Forecast')\n",
        "axs[0].set_xlabel('Date', fontsize=12)\n",
        "axs[0].set_ylabel('Avg Compound Sentiment', fontsize=12)\n",
        "axs[0].set_title('VADER Sentiment Trend & Forecast', fontsize=14)\n",
        "axs[0].set_ylim(-1, 1)\n",
        "axs[0].axhline(y=0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[0].axhline(y=-0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[0].grid(True)\n",
        "axs[0].legend(fontsize=10)\n",
        "axs[0].text(daily_vader['date'].iloc[-1], 0.07, 'Positive', color='green', fontsize=10)\n",
        "axs[0].text(daily_vader['date'].iloc[-1], 0.00, 'Neutral', color='blue', fontsize=10)\n",
        "axs[0].text(daily_vader['date'].iloc[-1], -0.09, 'Negative', color='red', fontsize=10)\n",
        "\n",
        "# BERT Plot\n",
        "axs[1].plot(daily_bert['date'], daily_bert['avg_compound'], marker='o', label='Historical')\n",
        "axs[1].plot(future_bert['date'], future_bert['predicted_compound'], marker='x', linestyle='--', label='Forecast')\n",
        "axs[1].set_xlabel('Date', fontsize=12)\n",
        "axs[1].set_title('BERT Sentiment Trend & Forecast', fontsize=14)\n",
        "axs[1].set_ylim(-1, 1)\n",
        "axs[1].axhline(y=0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[1].axhline(y=-0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[1].grid(True)\n",
        "axs[1].legend(fontsize=10)\n",
        "axs[1].text(daily_bert['date'].iloc[-1], 0.07, 'Positive', color='green', fontsize=10)\n",
        "axs[1].text(daily_bert['date'].iloc[-1], 0.00, 'Neutral', color='blue', fontsize=10)\n",
        "axs[1].text(daily_bert['date'].iloc[-1], -0.09, 'Negative', color='red', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Figure 2: Scatter Plot Comparing VADER vs. BERT for Each Post\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['vader_compound'], df['bert_compound'], alpha=0.6)\n",
        "plt.xlabel('VADER Compound Score', fontsize=12)\n",
        "plt.ylabel('BERT Compound Score', fontsize=12)\n",
        "plt.title('VADER vs. BERT Sentiment Scores', fontsize=14)\n",
        "plt.grid(True)\n",
        "lims = [-1, 1]\n",
        "plt.plot(lims, lims, 'r--', linewidth=1)  # Diagonal reference line\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Figure 3: Histograms of Sentiment Distributions for VADER and BERT\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
        "\n",
        "ax1.hist(df['vader_compound'], bins=20, color='skyblue', edgecolor='black')\n",
        "ax1.set_title('VADER Sentiment Distribution', fontsize=14)\n",
        "ax1.set_xlabel('VADER Compound Score', fontsize=12)\n",
        "ax1.set_ylabel('Frequency', fontsize=12)\n",
        "ax1.set_xlim(-1, 1)\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.hist(df['bert_compound'], bins=20, color='salmon', edgecolor='black')\n",
        "ax2.set_title('BERT Sentiment Distribution', fontsize=14)\n",
        "ax2.set_xlabel('BERT Compound Score', fontsize=12)\n",
        "ax2.set_xlim(-1, 1)\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Summary Statistics & Analysis Output\n",
        "# -------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📅 Additional Visualizations: Weekly Patterns & Statistical Summary\n",
        "\n",
        "To explore **temporal patterns** in sentiment, we break down scores by **day of the week** and analyze overall statistics across the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### 📦 Visualization: Box Plots of Sentiment Scores by Day of Week\n",
        "\n",
        "These two side-by-side box plots show the **distribution of daily sentiment** (VADER and BERT) across the 7 days of the week.\n",
        "\n",
        "- Each box represents the **spread and median** of compound sentiment scores for that day.\n",
        "- Helps identify:\n",
        "  - Are there days where posts are more emotionally intense?\n",
        "  - Do certain days tend to be more negative or positive?\n",
        "\n",
        "This is useful for seeing if emotional tone fluctuates around weekdays/weekends or during heightened activity days (e.g., town halls, news releases).\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Visualization: Stacked Bar Chart of VADER Sentiment Categories by Day\n",
        "\n",
        "This stacked bar chart visualizes the **volume of posts categorized as Positive, Neutral, or Negative** using VADER — grouped by day of the week.\n",
        "\n",
        "- Provides a **categorical breakdown** of how emotions are distributed over time.\n",
        "- Useful to identify:\n",
        "  - Which day sees the most negative posts?\n",
        "  - Which days foster positive or neutral discussions?\n",
        "\n",
        "---\n",
        "\n",
        "### 📈 Summary Statistics\n",
        "\n",
        "A few key statistics help summarize the overall emotional profile of the posts:\n",
        "\n",
        "- **Total posts analyzed**: Gives the dataset scope.\n",
        "- **Average compound score** (VADER and BERT): Measures general tone — closer to 0 = more neutral.\n",
        "- **Standard deviation**: Measures variability in emotional tone.\n",
        "- **Sentiment category counts**: How many posts are Positive, Neutral, or Negative.\n",
        "- **Correlation between VADER and BERT scores**:\n",
        "  - Indicates **how aligned the two models are** in their sentiment evaluations.\n",
        "  - A strong positive correlation (close to 1) means consistent scoring.\n",
        "\n",
        "This section supports both a **broad overview** and **granular insights**, combining visual and numeric analysis of the community’s emotional tone.\n"
      ],
      "metadata": {
        "id": "OhVZUjAF7Mf5"
      },
      "id": "OhVZUjAF7Mf5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf3ae7e",
      "metadata": {
        "id": "0cf3ae7e"
      },
      "outputs": [],
      "source": [
        "# Additional Visualizations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Visualization 4: Box Plots of VADER and BERT Compound Scores by Day of Week ---\n",
        "\n",
        "# Create a 'day_of_week' column in the DataFrame with proper ordering\n",
        "days_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "df['day_of_week'] = pd.Categorical(df['timestamp'].dt.day_name(), categories=days_order, ordered=True)\n",
        "\n",
        "# Prepare the data lists for each day for both VADER and BERT scores\n",
        "vader_box_data = [df.loc[df['day_of_week'] == day, 'vader_compound'].dropna() for day in days_order]\n",
        "bert_box_data = [df.loc[df['day_of_week'] == day, 'bert_compound'].dropna() for day in days_order]\n",
        "\n",
        "# Create side-by-side box plots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "axs[0].boxplot(vader_box_data, labels=days_order)\n",
        "axs[0].set_title(\"VADER Compound Scores by Day of Week\")\n",
        "axs[0].set_xlabel(\"Day of Week\")\n",
        "axs[0].set_ylabel(\"Compound Score\")\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].boxplot(bert_box_data, labels=days_order)\n",
        "axs[1].set_title(\"BERT Compound Scores by Day of Week\")\n",
        "axs[1].set_xlabel(\"Day of Week\")\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Visualization 5: Stacked Bar Chart of VADER Sentiment Categories by Day of Week ---\n",
        "\n",
        "# Define a helper function to assign sentiment categories for VADER\n",
        "def categorize(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Apply the function to create a new category column for VADER scores\n",
        "df['vader_category'] = df['vader_compound'].apply(categorize)\n",
        "\n",
        "# Create a pivot table: counts of sentiment categories by day of week\n",
        "pivot = df.groupby('day_of_week')['vader_category'].value_counts().unstack().fillna(0).loc[days_order]\n",
        "\n",
        "# Plot a stacked bar chart for the VADER sentiment categories by day\n",
        "pivot.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
        "plt.title(\"VADER Sentiment Categories by Day of Week\")\n",
        "plt.xlabel(\"Day of Week\")\n",
        "plt.ylabel(\"Number of Posts\")\n",
        "plt.legend(title=\"Sentiment Category\")\n",
        "plt.grid(True, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Summary Statistics:\")\n",
        "\n",
        "total_posts = len(df)\n",
        "print(f\"Total posts analyzed: {total_posts}\")\n",
        "\n",
        "avg_vader = df['vader_compound'].mean()\n",
        "avg_bert = df['bert_compound'].mean()\n",
        "std_vader = df['vader_compound'].std()\n",
        "std_bert = df['bert_compound'].std()\n",
        "\n",
        "print(f\"Average VADER compound score: {avg_vader:.3f}\")\n",
        "print(f\"Average BERT compound score: {avg_bert:.3f}\")\n",
        "print(f\"Standard Deviation VADER compound score: {std_vader:.3f}\")\n",
        "print(f\"Standard Deviation BERT compound score: {std_bert:.3f}\")\n",
        "\n",
        "# Define a helper function to assign a sentiment category\n",
        "def categorize(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "vader_categories = df['vader_compound'].apply(categorize)\n",
        "bert_categories = df['bert_compound'].apply(categorize)\n",
        "\n",
        "vader_counts = vader_categories.value_counts()\n",
        "bert_counts = bert_categories.value_counts()\n",
        "\n",
        "print(\"\\nVADER Sentiment Category Counts:\")\n",
        "print(vader_counts)\n",
        "print(\"\\nBERT Sentiment Category Counts:\")\n",
        "print(bert_counts)\n",
        "\n",
        "corr = df['vader_compound'].corr(df['bert_compound'])\n",
        "print(f\"\\nCorrelation between VADER and BERT compound scores: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "685ddc30",
      "metadata": {
        "id": "685ddc30"
      },
      "source": [
        "## Summary Statistics:\n",
        "\n",
        "- **Total posts analyzed**: 845  \n",
        "- **Average VADER compound score**: 0.079  \n",
        "- **Average BERT compound score**: -0.304  \n",
        "- **Standard Deviation VADER compound score**: 0.452  \n",
        "- **Standard Deviation BERT compound score**: 0.926  \n",
        "\n",
        "---\n",
        "\n",
        "### VADER Sentiment Category Counts:\n",
        "\n",
        "| Sentiment | Count |\n",
        "|-----------|-------|\n",
        "| Positive  | 350   |\n",
        "| Neutral   | 272   |\n",
        "| Negative  | 223   |\n",
        "\n",
        "---\n",
        "\n",
        "### BERT Sentiment Category Counts:\n",
        "\n",
        "| Sentiment | Count |\n",
        "|-----------|-------|\n",
        "| Negative  | 552   |\n",
        "| Positive  | 293   |\n",
        "\n",
        "## Insights & Recommendations\n",
        "- **Context Matters:**  \n",
        "  The slightly positive averages hide variability. BERT’s higher negative count may better reflect community distress.\n",
        "  \n",
        "- **Multiple Methods:**  \n",
        "  Using both VADER and BERT provides a fuller picture. Consider fine-tuning BERT on domain-specific data for improved accuracy.\n",
        "\n",
        "- **Additional Visualizations:**  \n",
        "  - **Heatmaps/Calendar Plots:** Identify periods with heightened negative sentiment.  \n",
        "  - **Topic Modeling:** Use LDA to link themes (e.g., water quality, cancer) with sentiment trends.  \n",
        "  - **Event Annotations:** Overlay key community events on trend graphs to contextualize shifts in sentiment.\n",
        "\n",
        "## Conclusion\n",
        "While overall sentiment appears slightly positive, the variability and methodological differences (especially BERT’s detection of more negative sentiment) suggest that the community's emotional response is more nuanced and polarized. This underscores the need for a combined quantitative and qualitative approach to fully capture the community's experiences, particularly given the serious issues of water quality and health.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}