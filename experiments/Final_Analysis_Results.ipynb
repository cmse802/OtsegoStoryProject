{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uzairname/OtsegoStoryProject/blob/main/experiments/Final_Analysis_Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to view the data, it is available at https://docs.google.com/spreadsheets/d/1WMoeCGweQA0xUtEwlK8R_XzRuZOgPqpHQDGWizb9slQ/"
      ],
      "metadata": {
        "id": "sX97uNt4ZBKi"
      },
      "id": "sX97uNt4ZBKi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to use Gemma, follow the instructions that show up after running the cell below to create a hugging face API key, and enter it when prompted.\n",
        "\n",
        "If asked to add the token as a git credential, type n."
      ],
      "metadata": {
        "id": "SDRSfp_RMlqU"
      },
      "id": "SDRSfp_RMlqU"
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login"
      ],
      "metadata": {
        "id": "ekspuZ_-MMQJ"
      },
      "id": "ekspuZ_-MMQJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bertopic bitsandbytes accelerate -q"
      ],
      "metadata": {
        "id": "sqo6lfLvrrOh"
      },
      "id": "sqo6lfLvrrOh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "ZBTbYBOo9cv7"
      },
      "id": "ZBTbYBOo9cv7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaca5acc",
      "metadata": {
        "id": "aaca5acc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "from datetime import timedelta\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Data Loading & Preprocessing\n",
        "# -------------------------------\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load CSV\n",
        "csv_url = f'https://docs.google.com/spreadsheets/d/1WMoeCGweQA0xUtEwlK8R_XzRuZOgPqpHQDGWizb9slQ/export?format=csv'\n",
        "df = pd.read_csv(csv_url)\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "df = df[df['timestamp'].notnull()]  # Remove rows with invalid timestamps\n",
        "df['content'] = df['content'].fillna(\"\").astype(str)\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # Replace newlines with a period and a space\n",
        "    text = re.sub(r'[\\r\\n]+', '. ', text)\n",
        "\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Filter out short sentences\n",
        "    filtered_sentences = [s for s in sentences if len(s) >= 20]\n",
        "\n",
        "    # Remove unwanted characters but preserve punctuation: , \" . ? !\n",
        "    cleaned_sentences = [\n",
        "        re.sub(r'[^a-zA-Z0-9\\s,.!?\\\"\\'’]', '', s)\n",
        "        for s in filtered_sentences\n",
        "    ]\n",
        "\n",
        "    # Optionally: lowercase everything for consistency\n",
        "    cleaned_sentences = [s.lower() for s in cleaned_sentences]\n",
        "\n",
        "    return ' '.join(cleaned_sentences)\n",
        "\n",
        "# Remove \"See more\"\n",
        "df['content'] = df['content'].str.replace(\"\\nSee more\", \"\", regex=False)\n",
        "\n",
        "# Apply the preprocessing function to the content column\n",
        "df['cleaned_content'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "# remove empty rows\n",
        "df = df[df['cleaned_content'] != '']\n",
        "\n",
        "# Inspect the first few rows of the new column\n",
        "print(df[['content', 'cleaned_content']].head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df['content'].str.len() < 21)]"
      ],
      "metadata": {
        "id": "pY7xyu-HOvj2"
      },
      "id": "pY7xyu-HOvj2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Sentiment Analysis\n",
        "\n",
        "The following cell computes sentiment scores via BERT and Vader. It may take a few minutes"
      ],
      "metadata": {
        "id": "Fy_D_XIUJq8Q"
      },
      "id": "Fy_D_XIUJq8Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55016140",
      "metadata": {
        "id": "55016140"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# 2. Sentiment Analysis: VADER\n",
        "# -------------------------------\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "df['vader_compound'] = df['content'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Sentiment Analysis: BERT\n",
        "# -------------------------------\n",
        "# Initialize a BERT sentiment-analysis pipeline.\n",
        "\n",
        "\n",
        "bert_pipeline = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "def get_bert_compound(text):\n",
        "    # Get the sentiment result (returns a list of dicts)\n",
        "    result = bert_pipeline(text)\n",
        "    # Extract the star rating from the label (e.g., \"4 stars\")\n",
        "    label = result[0]['label']\n",
        "    rating = int(label.split()[0])\n",
        "    # Map rating (1-5) to a compound score between -1 and 1:\n",
        "    # 1 -> -1.0, 2 -> -0.5, 3 -> 0.0, 4 -> 0.5, 5 -> 1.0\n",
        "    compound = (rating - 3) / 2.0\n",
        "    return compound\n",
        "\n",
        "# Apply BERT sentiment analysis (this may take a bit, depending on the dataset size)\n",
        "# df['bert_compound'] = df['content'].progress_apply(get_bert_compound)\n",
        "\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "df['sentiment'] = df['content'].progress_apply(lambda x: sentiment_pipeline(x)[0]['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit Topic Model\n",
        "\n",
        "The following cells train a topic model on the posts. It assigns a topic index to each post. We can then analyze the posts by topic, date, and sentiment."
      ],
      "metadata": {
        "id": "7WVSI3ypJbF4"
      },
      "id": "7WVSI3ypJbF4"
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import HDBSCAN\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from umap import UMAP\n",
        "\n",
        "model = BERTopic(\n",
        "    embedding_model=\"all-MiniLM-L6-v2\",\n",
        "    top_n_words=10,\n",
        "    min_topic_size=10,\n",
        "    n_gram_range=(1, 2),\n",
        "    vectorizer_model=CountVectorizer(ngram_range=(1, 2), stop_words=\"english\"),\n",
        "    representation_model=KeyBERTInspired(),\n",
        "    hdbscan_model= HDBSCAN(min_cluster_size=20),\n",
        "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine'),\n",
        "    verbose=True,\n",
        "    nr_topics=6,\n",
        ")\n",
        "\n",
        "topics, probs = model.fit_transform(df['cleaned_content'])\n",
        "df['topic'] = topics\n"
      ],
      "metadata": {
        "id": "cogGvM3grprg"
      },
      "id": "cogGvM3grprg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Topic Names\n",
        "\n",
        "The following cells will use a language model to assign the topics short titles in natural language. This is a free, open source model from hugging face. It may take a minute to download the model."
      ],
      "metadata": {
        "id": "1I8ctV2AJETk"
      },
      "id": "1I8ctV2AJETk"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# client = OpenAI()\n",
        "# def chat(prompt):\n",
        "#   response = client.responses.create(\n",
        "#       model=\"gpt-4o-mini\",\n",
        "#       input=prompt\n",
        "#   )\n",
        "#.  return response.output_text\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\")"
      ],
      "metadata": {
        "id": "KZj9S2LaCBMr"
      },
      "id": "KZj9S2LaCBMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(prompt):\n",
        "  from transformers import pipeline\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "  output = pipe(messages)\n",
        "  return output[0]['generated_text'][-1]['content']\n",
        "\n",
        "def get_topic_info(model, topic_id, df, n_docs=10):\n",
        "  topic_docs = df[df['topic']==topic_id]\n",
        "  topic_docs = topic_docs.sample(min(n_docs, len(topic_docs)))\n",
        "  keywords = ', '.join([i for i in model.generate_topic_labels(nr_words=5) if int(i.split('_')[0]) == topic_id][0].split('_')[1:])\n",
        "  return topic_docs['content'].tolist(), keywords\n"
      ],
      "metadata": {
        "id": "jMhmmwVFJ8Bk"
      },
      "id": "jMhmmwVFJ8Bk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "topic_names = {}\n",
        "for topic_id in list(set(model.topics_)):\n",
        "  docs, keywords = get_topic_info(model, topic_id, df)\n",
        "\n",
        "  docs_str = \"\\n\".join(docs)\n",
        "\n",
        "  prompt = docs_str + f\"\\n\\nYou are an AI topic model. Above are a subset of facebook posts from a community environmental health advocacy group in Otsego, MI that follow a common theme. The theme has the key words \\\"{keywords}\\\". Please come up with a clear and accurate name representing the subject of what people are discussing in 5 words or less. Respond with just the name and nothing else.\"\n",
        "\n",
        "  print(prompt)\n",
        "\n",
        "  response = chat(prompt)\n",
        "  print(response)\n",
        "\n",
        "  topic_names[topic_id]=response.strip()\n",
        "\n",
        "df['topic_name'] = df['topic'].map(topic_names)\n"
      ],
      "metadata": {
        "id": "KV4Y_8rHuD4W"
      },
      "id": "KV4Y_8rHuD4W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JvXLs0YAILPp"
      },
      "id": "JvXLs0YAILPp"
    },
    {
      "cell_type": "code",
      "source": [
        "df['topic_name'].value_counts()"
      ],
      "metadata": {
        "id": "oImYJyYfvZhb"
      },
      "id": "oImYJyYfvZhb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Figures"
      ],
      "metadata": {
        "id": "YNk5SN9jPM6J"
      },
      "id": "YNk5SN9jPM6J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment by topic"
      ],
      "metadata": {
        "id": "5XD3xuD_POpQ"
      },
      "id": "5XD3xuD_POpQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Determine mean sentiment by topics\n",
        "df.groupby('topic_name').agg({'vader_compound': 'mean'})\n",
        "\n",
        "# Determine mean sentiment by topics over time\n",
        "sentiment_by_topic_year = df.groupby(['topic_name', df['timestamp'].dt.year]).agg({'vader_compound': 'mean', 'bert_compound': 'mean'})\n",
        "\n",
        "sentiment_by_topic_year = sentiment_by_topic_year.reset_index()\n",
        "\n",
        "# Create the Plotly line plot\n",
        "fig = px.line(\n",
        "    sentiment_by_topic_year,\n",
        "    x='timestamp',\n",
        "    y=['vader_compound'],\n",
        "    color='topic_name',\n",
        "    title='Mean Sentiment by Topic Over Time',\n",
        "    labels={'timestamp': 'Year', 'value': 'Mean Sentiment', 'topic_name': 'Topic'},\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "7iuxmB_6Dyca"
      },
      "id": "7iuxmB_6Dyca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XuDpkvzPUVZ"
      },
      "id": "2XuDpkvzPUVZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 4. Daily Aggregation & Forecasting\n",
        "# -------------------------------\n",
        "# Create a date column\n",
        "df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "# Aggregate daily average sentiment for VADER and BERT\n",
        "daily_vader = df.groupby('date')['vader_compound'].mean().reset_index().rename(columns={'vader_compound': 'avg_compound'})\n",
        "daily_bert = df.groupby('date')['bert_compound'].mean().reset_index().rename(columns={'bert_compound': 'avg_compound'})\n",
        "df['day_mean_vader'] = df.groupby('date')['vader_compound'].transform('mean')\n",
        "df['day_mean_bert'] = df.groupby('date')['bert_compound'].transform('mean')\n",
        "\n",
        "\n",
        "# Forecast function using linear regression\n",
        "def forecast_sentiment():\n",
        "    # Convert dates to ordinal numbers for regression\n",
        "    daily_df['date_ordinal'] = pd.to_datetime(daily_df['date']).apply(lambda date: date.toordinal())\n",
        "    X = daily_df['date_ordinal'].values.reshape(-1, 1)\n",
        "    y = daily_df['avg_compound'].values\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Forecast from the day after the last date until the end of 2026\n",
        "    last_date = pd.to_datetime(daily_df['date'].max())\n",
        "    future_dates = pd.date_range(start=last_date + timedelta(days=1), end=pd.Timestamp(\"2026-12-31\"))\n",
        "    future_ordinals = np.array([d.toordinal() for d in future_dates]).reshape(-1, 1)\n",
        "    predicted = model.predict(future_ordinals)\n",
        "    future_df = pd.DataFrame({'date': future_dates, 'predicted_compound': predicted})\n",
        "    return future_df\n",
        "\n",
        "vader_forecast = forecast_sentiment(df[['date', 'day_mean_vader']].rename(columns={'day_mean_vader': 'avg_compound'}))\n",
        "bert_forecast = forecast_sentiment(df[['date', 'day_mean_bert']].rename(columns={'day_mean_bert': 'avg_compound'}))\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Visualization\n",
        "# -------------------------------\n",
        "\n",
        "# Figure 1: Trend Plots (side-by-side) for VADER and BERT with Forecasts\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "# VADER Plot\n",
        "axs[0].plot(daily_vader['date'], df['avg_compound'], marker='o', label='Historical')\n",
        "axs[0].plot(vader_forecast['date'], vader_forecast['predicted_compound'], marker='x', linestyle='--', label='Forecast')\n",
        "axs[0].set_xlabel('Date', fontsize=12)\n",
        "axs[0].set_ylabel('Avg Compound Sentiment', fontsize=12)\n",
        "axs[0].set_title('VADER Sentiment Trend & Forecast', fontsize=14)\n",
        "axs[0].set_ylim(-1, 1)\n",
        "axs[0].axhline(y=0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[0].axhline(y=-0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[0].grid(True)\n",
        "axs[0].legend(fontsize=10)\n",
        "axs[0].text(daily_vader['date'].iloc[-1], 0.07, 'Positive', color='green', fontsize=10)\n",
        "axs[0].text(daily_vader['date'].iloc[-1], 0.00, 'Neutral', color='blue', fontsize=10)\n",
        "axs[0].text(daily_vader['date'].iloc[-1], -0.09, 'Negative', color='red', fontsize=10)\n",
        "\n",
        "# BERT Plot\n",
        "axs[1].plot(daily_bert['date'], daily_bert['avg_compound'], marker='o', label='Historical')\n",
        "axs[1].plot(bert_forecast['date'], bert_forecast['predicted_compound'], marker='x', linestyle='--', label='Forecast')\n",
        "axs[1].set_xlabel('Date', fontsize=12)\n",
        "axs[1].set_title('BERT Sentiment Trend & Forecast', fontsize=14)\n",
        "axs[1].set_ylim(-1, 1)\n",
        "axs[1].axhline(y=0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[1].axhline(y=-0.05, color='gray', linestyle='--', linewidth=1)\n",
        "axs[1].grid(True)\n",
        "axs[1].legend(fontsize=10)\n",
        "axs[1].text(daily_bert['date'].iloc[-1], 0.07, 'Positive', color='green', fontsize=10)\n",
        "axs[1].text(daily_bert['date'].iloc[-1], 0.00, 'Neutral', color='blue', fontsize=10)\n",
        "axs[1].text(daily_bert['date'].iloc[-1], -0.09, 'Negative', color='red', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Figure 2: Scatter Plot Comparing VADER vs. BERT for Each Post\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['vader_compound'], df['bert_compound'], alpha=0.6)\n",
        "plt.xlabel('VADER Compound Score', fontsize=12)\n",
        "plt.ylabel('BERT Compound Score', fontsize=12)\n",
        "plt.title('VADER vs. BERT Sentiment Scores', fontsize=14)\n",
        "plt.grid(True)\n",
        "lims = [-1, 1]\n",
        "plt.plot(lims, lims, 'r--', linewidth=1)  # Diagonal reference line\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Figure 3: Histograms of Sentiment Distributions for VADER and BERT\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
        "\n",
        "ax1.hist(df['vader_compound'], bins=20, color='skyblue', edgecolor='black')\n",
        "ax1.set_title('VADER Sentiment Distribution', fontsize=14)\n",
        "ax1.set_xlabel('VADER Compound Score', fontsize=12)\n",
        "ax1.set_ylabel('Frequency', fontsize=12)\n",
        "ax1.set_xlim(-1, 1)\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.hist(df['bert_compound'], bins=20, color='salmon', edgecolor='black')\n",
        "ax2.set_title('BERT Sentiment Distribution', fontsize=14)\n",
        "ax2.set_xlabel('BERT Compound Score', fontsize=12)\n",
        "ax2.set_xlim(-1, 1)\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Summary Statistics & Analysis Output\n",
        "# -------------------------------"
      ],
      "metadata": {
        "id": "RRZlokpIzXJF"
      },
      "id": "RRZlokpIzXJF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf3ae7e",
      "metadata": {
        "id": "0cf3ae7e"
      },
      "outputs": [],
      "source": [
        "# Additional Visualizations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Visualization 4: Box Plots of VADER and BERT Compound Scores by Day of Week ---\n",
        "\n",
        "# Create a 'day_of_week' column in the DataFrame with proper ordering\n",
        "days_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "df['day_of_week'] = pd.Categorical(df['timestamp'].dt.day_name(), categories=days_order, ordered=True)\n",
        "\n",
        "# Prepare the data lists for each day for both VADER and BERT scores\n",
        "vader_box_data = [df.loc[df['day_of_week'] == day, 'vader_compound'].dropna() for day in days_order]\n",
        "bert_box_data = [df.loc[df['day_of_week'] == day, 'bert_compound'].dropna() for day in days_order]\n",
        "\n",
        "# Create side-by-side box plots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "axs[0].boxplot(vader_box_data, labels=days_order)\n",
        "axs[0].set_title(\"VADER Compound Scores by Day of Week\")\n",
        "axs[0].set_xlabel(\"Day of Week\")\n",
        "axs[0].set_ylabel(\"Compound Score\")\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].boxplot(bert_box_data, labels=days_order)\n",
        "axs[1].set_title(\"BERT Compound Scores by Day of Week\")\n",
        "axs[1].set_xlabel(\"Day of Week\")\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Visualization 5: Stacked Bar Chart of VADER Sentiment Categories by Day of Week ---\n",
        "\n",
        "# Define a helper function to assign sentiment categories for VADER\n",
        "def categorize(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Apply the function to create a new category column for VADER scores\n",
        "df['vader_category'] = df['vader_compound'].apply(categorize)\n",
        "\n",
        "# Create a pivot table: counts of sentiment categories by day of week\n",
        "pivot = df.groupby('day_of_week')['vader_category'].value_counts().unstack().fillna(0).loc[days_order]\n",
        "\n",
        "# Plot a stacked bar chart for the VADER sentiment categories by day\n",
        "pivot.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
        "plt.title(\"VADER Sentiment Categories by Day of Week\")\n",
        "plt.xlabel(\"Day of Week\")\n",
        "plt.ylabel(\"Number of Posts\")\n",
        "plt.legend(title=\"Sentiment Category\")\n",
        "plt.grid(True, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Summary Statistics:\")\n",
        "\n",
        "total_posts = len(df)\n",
        "print(f\"Total posts analyzed: {total_posts}\")\n",
        "\n",
        "avg_vader = df['vader_compound'].mean()\n",
        "avg_bert = df['bert_compound'].mean()\n",
        "std_vader = df['vader_compound'].std()\n",
        "std_bert = df['bert_compound'].std()\n",
        "\n",
        "print(f\"Average VADER compound score: {avg_vader:.3f}\")\n",
        "print(f\"Average BERT compound score: {avg_bert:.3f}\")\n",
        "print(f\"Standard Deviation VADER compound score: {std_vader:.3f}\")\n",
        "print(f\"Standard Deviation BERT compound score: {std_bert:.3f}\")\n",
        "\n",
        "# Define a helper function to assign a sentiment category\n",
        "def categorize(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "vader_categories = df['vader_compound'].apply(categorize)\n",
        "bert_categories = df['bert_compound'].apply(categorize)\n",
        "\n",
        "vader_counts = vader_categories.value_counts()\n",
        "bert_counts = bert_categories.value_counts()\n",
        "\n",
        "print(\"\\nVADER Sentiment Category Counts:\")\n",
        "print(vader_counts)\n",
        "print(\"\\nBERT Sentiment Category Counts:\")\n",
        "print(bert_counts)\n",
        "\n",
        "corr = df['vader_compound'].corr(df['bert_compound'])\n",
        "print(f\"\\nCorrelation between VADER and BERT compound scores: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "685ddc30",
      "metadata": {
        "id": "685ddc30"
      },
      "source": [
        "# Sentiment Analysis Summary\n",
        "\n",
        "**Total Posts Analyzed:** 256\n",
        "\n",
        "## Overall Sentiment\n",
        "- **VADER Average Compound:** 0.086  \n",
        "- **BERT Average Compound:** 0.092  \n",
        "*Both methods show slightly positive averages, but these can mask underlying complexity.*\n",
        "\n",
        "- **VADER Std. Dev.:** 0.449  \n",
        "- **BERT Std. Dev.:** 0.800  \n",
        "*BERT exhibits a wider range, indicating more extreme sentiment values.*\n",
        "\n",
        "## Sentiment Categories\n",
        "- **VADER Counts:**\n",
        "  - Positive: 110\n",
        "  - Neutral: 84\n",
        "  - Negative: 62\n",
        "\n",
        "- **BERT Counts:**\n",
        "  - Positive: 117\n",
        "  - Negative: 91\n",
        "  - Neutral: 48\n",
        "\n",
        "*BERT flags more negative posts, suggesting higher sensitivity to distress.*\n",
        "\n",
        "## Correlation Between Methods\n",
        "- **Correlation:** 0.437  \n",
        "*Moderate correlation indicates the two methods capture sentiment differently.*\n",
        "\n",
        "## Insights & Recommendations\n",
        "- **Context Matters:**  \n",
        "  The slightly positive averages hide variability. BERT’s higher negative count may better reflect community distress.\n",
        "  \n",
        "- **Multiple Methods:**  \n",
        "  Using both VADER and BERT provides a fuller picture. Consider fine-tuning BERT on domain-specific data for improved accuracy.\n",
        "\n",
        "- **Additional Visualizations:**  \n",
        "  - **Heatmaps/Calendar Plots:** Identify periods with heightened negative sentiment.  \n",
        "  - **Topic Modeling:** Use LDA to link themes (e.g., water quality, cancer) with sentiment trends.  \n",
        "  - **Event Annotations:** Overlay key community events on trend graphs to contextualize shifts in sentiment.\n",
        "\n",
        "## Conclusion\n",
        "While overall sentiment appears slightly positive, the variability and methodological differences (especially BERT’s detection of more negative sentiment) suggest that the community's emotional response is more nuanced and polarized. This underscores the need for a combined quantitative and qualitative approach to fully capture the community's experiences, particularly given the serious issues of water quality and health.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca902eb",
      "metadata": {
        "id": "5ca902eb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}