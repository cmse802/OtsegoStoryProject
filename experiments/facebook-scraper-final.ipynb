{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSU_J4O'S AMAZING SCRAPER!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Instructions\n",
    "Hey! Before running this scraper, make sure you’ve completed the following:\n",
    "\n",
    "1. **Install the correct ChromeDriver version**\n",
    "   - ChromeDriver must match your currently installed version of Google Chrome.\n",
    "   - To check your version: open `chrome://settings/help` in your browser.\n",
    "   - Download the corresponding ChromeDriver from: [https://chromedriver.chromium.org/downloads](https://chromedriver.chromium.org/downloads)\n",
    "   - Make sure the path to ChromeDriver is correctly set in `CHROMEDRIVER_PATH` inside the code.\n",
    "       - THIS IS IN YOUR DOWNLOADS FOLDER. PLEASE KEEP CHROMEDRIVER IN YOUR DOWNLOADS FOLDER.\n",
    "\n",
    "2. **Set up a Python virtual environment** (recommended)\n",
    "   ```bash\n",
    "   python -m venv venv\n",
    "   source venv/bin/activate  # On Windows use: venv\\Scripts\\activate\n",
    "   ```\n",
    "\n",
    "3. **Install dependencies from `requirements.txt`**\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Standard Library ===\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin\n",
    "from pathlib import Path\n",
    "import platform\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHY DID WE MAKE OUR OWN SCRAPER?\n",
    "We originally tried using two libraries:\n",
    "- [facebook-scraper](https://github.com/kevinzg/facebook-scraper)\n",
    "- [facebook-page-scraper](https://pypi.org/project/facebook-page-scraper/)\n",
    "\n",
    "Unfortunately, both of these tools were unable to bypass Facebook’s security measures (like login walls, dynamic content loading, etc.).\n",
    "\n",
    "By building our own scraper:\n",
    "- We can extract the data we need.\n",
    "- We bypass some protections using a real browser session and cookies.\n",
    "- And honestly, it's pretty cool that we made it ourselves as a group project\n",
    "\n",
    "---\n",
    "\n",
    "### Setup: ChromeDriver, Cookies, and Facebook Group Page\n",
    "- Loads the locally installed ChromeDriver and launches a new Chrome browser session.\n",
    "- Applies previously saved Facebook session cookies to bypass the login process.\n",
    "    - You can export your cookies using the [ExportThisCookie browser extension](https://exportthiscookie.com/)\n",
    "- Navigates directly to the specified Facebook group page to begin scraping.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chromedriver_path():\n",
    "    # Get user's Downloads folder\n",
    "    downloads = Path.home() / \"Downloads\"\n",
    "    \n",
    "    # Choose driver filename depending on OS\n",
    "    system = platform.system()\n",
    "    machine = platform.machine()\n",
    "\n",
    "    if system == \"Windows\":\n",
    "        filename = \"chromedriver.exe\"\n",
    "        driver_path = downloads / filename\n",
    "\n",
    "    elif system == \"Darwin\" and machine == \"x86_64\":  # Intel Mac\n",
    "        driver_path = downloads / \"chromedriver-mac-x64\" / \"chromedriver\"\n",
    "\n",
    "    else:\n",
    "        filename = \"chromedriver\"\n",
    "        driver_path = downloads / filename\n",
    "\n",
    "    if driver_path.exists():\n",
    "        return str(driver_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"ChromeDriver not found at {driver_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Path Configuration ===\n",
    "COOKIES_PATH = \"fb_cookies.json\"\n",
    "GROUP_URL = \"https://www.facebook.com/groups/1996906587229548/\"\n",
    "\n",
    "# === Set Up Chrome WebDriver ===\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Only set Chrome binary path if on macOS\n",
    "if platform.system() == \"Darwin\":\n",
    "    mac_chrome_path = \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\"\n",
    "    if Path(mac_chrome_path).exists():\n",
    "        options.binary_location = mac_chrome_path\n",
    "\n",
    "# Optional: uncomment this to run headless\n",
    "# options.add_argument(\"--headless\")\n",
    "\n",
    "# Automatically fetch correct driver\n",
    "service = Service(get_chromedriver_path())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# === Load Facebook and Inject Cookies ===\n",
    "driver.get(\"https://www.facebook.com/\")\n",
    "time.sleep(3)\n",
    "\n",
    "with open(COOKIES_PATH, \"r\") as f:\n",
    "    cookies = json.load(f)\n",
    "    for cookie in cookies:\n",
    "        if \"sameSite\" in cookie:\n",
    "            cookie[\"sameSite\"] = \"Strict\"\n",
    "        try:\n",
    "            driver.add_cookie(cookie)\n",
    "        except Exception as e:\n",
    "            print(\"cookie error:\", cookie.get(\"name\"), e)\n",
    "\n",
    "# === Navigate to Target Group ===\n",
    "driver.get(GROUP_URL)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What Did This Do?\n",
    "This section is your method to get into Facebook:\n",
    "- Without needing manual login, it uses saved cookies to impersonate a logged-in session.\n",
    "- This makes scraping faster, more consistent, and less likely to trigger Facebook’s security walls.\n",
    "- Once on the group page, the scraper can begin scanning for content like posts and timestamps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Scraper Function\n",
    "\n",
    "#### What It Does:\n",
    "- Automatically scrolls through the Facebook group page to load posts.\n",
    "- Extracts timestamp, post content, and post URL from each visible post.\n",
    "- Automatically stops if no new content is found after a couple scrolls.\n",
    "- Saves all scraped data into a clean, structured CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j4o's ultimate facebook group scraper v3.1 (cleaned on 4-8-2025)\n",
    "# features:\n",
    "# - relative timestamp conversion (3d → actual date)  (3-29-2025)\n",
    "# - error resilience against facebook's nonsense (3-29-2025)\n",
    "# - clean CSV output (3-30-2025)\n",
    "# - renamed for the group! (4-9-2025)\n",
    "\n",
    "# --------------------------\n",
    "# 1. SCROLLING MECHANISM\n",
    "# --------------------------\n",
    "def scroll_to_load(driver, max_scrolls=30, wait=2, min_scrolls=10, verbose=True):\n",
    "    \"\"\"scrolls the page with smart detection but ensures a minimum number of scrolls\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    stall_counter = 0\n",
    "\n",
    "    for i in range(max_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(wait)\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        posts_found = len(driver.find_elements(By.CSS_SELECTOR, \"div[role='article']\"))\n",
    "        if verbose:\n",
    "            print(f\"Scroll {i+1}/{max_scrolls} | Posts found: {posts_found}\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            stall_counter += 1\n",
    "            if i + 1 >= min_scrolls:\n",
    "                print(\"No new posts detected - stopping scroll\")\n",
    "                break\n",
    "        else:\n",
    "            stall_counter = 0\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "# --------------------------  \n",
    "# 2. TIMESTAMP CONVERSION\n",
    "# --------------------------\n",
    "def get_timestamp(post):\n",
    "    \"\"\"convert facebook's vague time hints into actual dates\"\"\"\n",
    "    time_elem = post.find(\"a\", href=lambda x: x and (\"/posts/\" in x or \"/permalink/\" in x))\n",
    "    if not time_elem:\n",
    "        return \"unknown\"\n",
    "\n",
    "    timestamp = time_elem.get_text(strip=True)\n",
    "\n",
    "    if \"Just now\" in timestamp:\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    match = re.match(r\"(\\d+)\\s*([smhdwy])\", timestamp)\n",
    "    if not match:\n",
    "        return timestamp\n",
    "\n",
    "    num, unit = int(match.group(1)), match.group(2)\n",
    "    now = datetime.now()\n",
    "\n",
    "    delta_map = {\n",
    "        's': timedelta(seconds=num),\n",
    "        'm': timedelta(minutes=num),\n",
    "        'h': timedelta(hours=num),\n",
    "        'd': timedelta(days=num),\n",
    "        'w': timedelta(weeks=num),\n",
    "        'y': timedelta(days=num * 365),\n",
    "    }\n",
    "\n",
    "    return (now - delta_map.get(unit, timedelta())).strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "# --------------------------\n",
    "# 3. MAIN EXTRACTION LOGIC\n",
    "# --------------------------\n",
    "def get_content(post):\n",
    "    \"\"\"extract post text with 'See more' expansion\"\"\"\n",
    "    content_div = post.find(\"div\", {\"dir\": \"auto\"})\n",
    "    if not content_div:\n",
    "        return \"\"\n",
    "\n",
    "    content = content_div.get_text(\"\\n\", strip=True)\n",
    "    see_more = post.find(\"div\", string=re.compile(r\"See\\\\s+more\", re.I))\n",
    "    if see_more:\n",
    "        more_text = see_more.find_next(\"div\")\n",
    "        if more_text:\n",
    "            content += \"\\n\" + more_text.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    return content\n",
    "\n",
    "def get_post_url(post):\n",
    "    \"\"\"extract post URL with fallback\"\"\"\n",
    "    time_elem = post.find(\"a\", href=lambda x: x and (\"/posts/\" in x or \"/permalink/\" in x))\n",
    "    if time_elem:\n",
    "        return urljoin(\"https://www.facebook.com\", time_elem['href'])\n",
    "    return \"unknown\"\n",
    "\n",
    "def extract_posts(driver):\n",
    "    \"\"\"More reliable post detection with Facebook's current layout\"\"\"\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    posts_data = []\n",
    "\n",
    "    post_selectors = [\n",
    "        {'role': 'article'},\n",
    "        {'data-pagelet': re.compile('FeedUnit_')},\n",
    "        {'class': re.compile('x1yztbdb')}\n",
    "    ]\n",
    "\n",
    "    for selector in post_selectors:\n",
    "        posts = soup.find_all(\"div\", selector)\n",
    "        if posts:\n",
    "            break\n",
    "\n",
    "    for post in posts:\n",
    "        try:\n",
    "            post_data = {\n",
    "                \"timestamp\": get_timestamp(post),\n",
    "                \"content\": get_content(post),\n",
    "                \"post_url\": get_post_url(post),\n",
    "            }\n",
    "            posts_data.append(post_data)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return posts_data\n",
    "\n",
    "# --------------------------\n",
    "# 4. OUTPUT FUNCTIONS\n",
    "# --------------------------\n",
    "def save_posts_to_csv(posts, filename=\"fb_scraped_data.csv\"):\n",
    "    \"\"\"save our hard-earned data to CSV\"\"\"\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"timestamp\", \"content\", \"post_url\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(posts)\n",
    "\n",
    "# --------------------------\n",
    "# 5. USER-FRIENDLY FILENAME HANDLING\n",
    "# --------------------------\n",
    "def get_output_filename():\n",
    "    while True:\n",
    "        user_input = input(\"Enter output filename (or press Enter for 'fb_scraped_data.csv'): \").strip()\n",
    "        if not user_input:\n",
    "            return \"fb_scraped_data.csv\"\n",
    "        if not user_input.lower().endswith('.csv'):\n",
    "            user_input += '.csv'\n",
    "        if len(user_input) > 100:\n",
    "            print(\"That filename is too long. Try something shorter.\")\n",
    "            continue\n",
    "        if not re.match(r'^[\\w\\-\\. ]+$', user_input):\n",
    "            print(\"Invalid characters in filename. Use only letters, numbers, spaces, hyphens, and periods.\")\n",
    "            continue\n",
    "        return user_input\n",
    "\n",
    "# --------------------------\n",
    "# 6. MAIN EXECUTION FLOW\n",
    "# --------------------------\n",
    "def run_scraper(driver):\n",
    "    try:\n",
    "        print(\"\\U0001f680 Starting Facebook scraper\")\n",
    "        scrolls_per_batch = 11\n",
    "\n",
    "        # === This is where you adjust the running time of the program ===\n",
    "        # Setting total run time to approximately 3 minutes\n",
    "        total_batches = 30  # 6 batches × ~30s each = ~3 minutes\n",
    "        wait = 3.0\n",
    "\n",
    "        all_posts = {}\n",
    "        stale_batches = 0\n",
    "\n",
    "        for batch in range(total_batches):\n",
    "            print(f\"Batch {batch+1}/{total_batches} | Scrolling {scrolls_per_batch} times...\")\n",
    "            scroll_to_load(driver, max_scrolls=scrolls_per_batch, wait=wait, verbose=False)\n",
    "\n",
    "            print(\"\\U0001f50d Extracting posts...\")\n",
    "            new_posts = extract_posts(driver)\n",
    "\n",
    "            new_count = 0\n",
    "            for p in new_posts:\n",
    "                if p[\"post_url\"] not in all_posts:\n",
    "                    all_posts[p[\"post_url\"]] = p\n",
    "                    new_count += 1\n",
    "\n",
    "            total_unique = len(all_posts)\n",
    "            print(f\"New: {new_count} | Total unique: {total_unique}\")\n",
    "\n",
    "            if new_count == 0:\n",
    "                stale_batches += 1\n",
    "                print(f\"\\u26a0\\ufe0f No new posts in this batch ({stale_batches}x)\")\n",
    "                if stale_batches >= 3:\n",
    "                    print(\"No new content in 3 batches. Stopping early.\")\n",
    "                    break\n",
    "            else:\n",
    "                stale_batches = 0\n",
    "\n",
    "        posts_list = list(all_posts.values())\n",
    "        print(f\"\\n✅ Finished scraping: {len(posts_list)} unique posts\")\n",
    "\n",
    "        if posts_list:\n",
    "            filename = get_output_filename()\n",
    "            save_posts_to_csv(posts_list, filename)\n",
    "            print(f\"Saved {len(posts_list)} posts to '{filename}'\")\n",
    "\n",
    "            print(\"\\nTop timestamps:\")\n",
    "            df = pd.DataFrame(posts_list)\n",
    "            print(df['timestamp'].value_counts().head(10))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Do I Make It Run Longer?\n",
    "\n",
    "Normally, the scraper runs for approximately 3 minutes, which is enough for testing or small-scale use.\n",
    "\n",
    "If you'd like to extend the scraping time:\n",
    "\n",
    "- Find this section in the `run_scraper` function:\n",
    "\n",
    "  ```python\n",
    "  # === This is where you adjust the running time of the program ===\n",
    "  total_batches = 6  # 6 batches × ~30s each = ~3 minutes\n",
    "    ```\n",
    "    \n",
    "#### Increase `total_batches` like this:\n",
    "- `12` → ~6 minutes  \n",
    "- `20` → ~10 minutes  \n",
    "- `30` → ~15 minutes+\n",
    "\n",
    "Each batch:\n",
    "- Scrolls the page 10 times (`scrolls_per_batch = 10`)\n",
    "- Waits ~2.5 seconds between scrolls\n",
    "\n",
    "> Longer runs **WILL** trigger Facebook's security measures and you may be rate-limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run The Scraper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Facebook scraper\n",
      "Batch 1/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 10 | Total unique: 10\n",
      "Batch 2/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 75 | Total unique: 85\n",
      "Batch 3/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 75 | Total unique: 160\n",
      "Batch 4/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 34 | Total unique: 194\n",
      "Batch 5/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 40 | Total unique: 234\n",
      "Batch 6/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 27 | Total unique: 261\n",
      "Batch 7/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 60 | Total unique: 321\n",
      "Batch 8/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 60 | Total unique: 381\n",
      "Batch 9/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 37 | Total unique: 418\n",
      "Batch 10/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 2 | Total unique: 420\n",
      "Batch 11/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 0 | Total unique: 420\n",
      "⚠️ No new posts in this batch (1x)\n",
      "Batch 12/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 2 | Total unique: 422\n",
      "Batch 13/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 7 | Total unique: 429\n",
      "Batch 14/30 | Scrolling 11 times...\n",
      "🔍 Extracting posts...\n",
      "New: 43 | Total unique: 472\n",
      "Batch 15/30 | Scrolling 11 times...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_scraper(driver)\n",
      "Cell \u001b[0;32mIn[24], line 167\u001b[0m, in \u001b[0;36mrun_scraper\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_batches):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Scrolling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscrolls_per_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m times...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 167\u001b[0m     scroll_to_load(driver, max_scrolls\u001b[38;5;241m=\u001b[39mscrolls_per_batch, wait\u001b[38;5;241m=\u001b[39mwait, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\U0001f50d\u001b[39;00m\u001b[38;5;124m Extracting posts...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m     new_posts \u001b[38;5;241m=\u001b[39m extract_posts(driver)\n",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m, in \u001b[0;36mscroll_to_load\u001b[0;34m(driver, max_scrolls, wait, min_scrolls, verbose)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscroll_to_load\u001b[39m(driver, max_scrolls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, min_scrolls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"scrolls the page with smart detection but ensures a minimum number of scrolls\"\"\"\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     last_height \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn document.body.scrollHeight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     stall_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_scrolls):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:878\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[0;34m(self, script, *args)\u001b[0m\n\u001b[1;32m    875\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[1;32m    876\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[0;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(command, {\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m'\u001b[39m: script,\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m: converted_args})[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:422\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    419\u001b[0m         params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m    421\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_value(params)\n\u001b[0;32m--> 422\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:421\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    419\u001b[0m data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdump_json(params)\n\u001b[1;32m    420\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:443\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    440\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 443\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    444\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_scraper(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running will save to your experiments folder, so you can just run the CSV below by inputting the filename."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
