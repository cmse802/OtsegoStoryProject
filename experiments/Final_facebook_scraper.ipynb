{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSU_J4O'S AMAZING SCRAPER!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Instructions\n",
    "Hey! Before running this scraper, make sure you’ve completed the following:\n",
    "\n",
    "1. **Install the correct ChromeDriver version**\n",
    "   - ChromeDriver must match your currently installed version of Google Chrome.\n",
    "   - To check your version: open `chrome://settings/help` in your browser.\n",
    "   - Download the corresponding ChromeDriver from: [https://chromedriver.chromium.org/downloads](https://chromedriver.chromium.org/downloads)\n",
    "   - Extract the chromedriver .zip file in your downloads folder.\n",
    "   - Make sure the path to ChromeDriver is correctly set in `CHROMEDRIVER_PATH` inside the code.\n",
    "       - THIS IS IN YOUR DOWNLOADS FOLDER. PLEASE KEEP CHROMEDRIVER IN YOUR DOWNLOADS FOLDER.\n",
    "\n",
    "2. **Set up a Python virtual environment** (recommended)\n",
    "   ```bash\n",
    "   python -m venv venv\n",
    "   source venv/bin/activate  # On Windows use: venv\\Scripts\\activate\n",
    "   ```\n",
    "\n",
    "3. **Install dependencies from `requirements.txt`**\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Standard Library ===\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin\n",
    "from pathlib import Path\n",
    "import platform\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHY DID WE MAKE OUR OWN SCRAPER?\n",
    "We originally tried using two libraries:\n",
    "- [facebook-scraper](https://github.com/kevinzg/facebook-scraper)\n",
    "- [facebook-page-scraper](https://pypi.org/project/facebook-page-scraper/)\n",
    "\n",
    "Unfortunately, both of these tools were unable to bypass Facebook’s security measures (like login walls, dynamic content loading, etc.).\n",
    "\n",
    "By building our own scraper:\n",
    "- We can extract the data we need.\n",
    "- We bypass some protections using a real browser session and cookies.\n",
    "- And honestly, it's pretty cool that we made it ourselves as a group project\n",
    "\n",
    "---\n",
    "\n",
    "### Setup: ChromeDriver, Cookies, and Facebook Group Page\n",
    "- Loads the locally installed ChromeDriver and launches a new Chrome browser session.\n",
    "- Applies previously saved Facebook session cookies to bypass the login process.\n",
    "    - First, you will need to use Firefox to export your cookies. Chrome has disabled the ability to export your own cookies with a recent update, but you will be able to do this on firefox.\n",
    "    - You can export your cookies using the [ExportThisCookie browser extension](https://exportthiscookie.com/)\n",
    "    - Once signed into facebook, open the extension and click \"Export cookies\" from the bottom right of the extension tab.\n",
    "    - Create a new txt file in this notebook's experiements folder and paste your cookies.\n",
    "    - Rename to \"fb_cookies.json\"\n",
    "    \n",
    "- Navigates directly to the specified Facebook group page to begin scraping.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chromedriver_path():\n",
    "    # Get user's Downloads folder\n",
    "    downloads = Path.home() / \"Downloads\"\n",
    "    \n",
    "    # Choose driver filename depending on OS\n",
    "    system = platform.system()\n",
    "    machine = platform.machine()\n",
    "\n",
    "    if system == \"Windows\":\n",
    "        filename = \"chromedriver\"\n",
    "        driver_path = downloads / \"chromedriver-win64\" / \"chromedriver-win64\" / filename\n",
    "\n",
    "    elif system == \"Darwin\" and machine == \"x86_64\":  # Intel Mac\n",
    "        driver_path = downloads / \"chromedriver-mac-x64\" / \"chromedriver\"\n",
    "\n",
    "    else:\n",
    "        filename = \"chromedriver\"\n",
    "        driver_path = downloads / \"chromedriver-mac-arm64\" /  filename\n",
    "\n",
    "    if driver_path.exists():\n",
    "        return str(driver_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"ChromeDriver not found at {driver_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Path Configuration ===\n",
    "COOKIES_PATH = \"fb_cookies.json\"\n",
    "GROUP_URL = \"https://www.facebook.com/groups/409901703293362/\"\n",
    "# this is the otsego group for our project\n",
    "# \"https://www.facebook.com/groups/1996906587229548/\"\n",
    "\n",
    "# === Set Up Chrome WebDriver ===\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Only set Chrome binary path if on macOS\n",
    "if platform.system() == \"Darwin\":\n",
    "    mac_chrome_path = \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\"\n",
    "    if Path(mac_chrome_path).exists():\n",
    "        options.binary_location = mac_chrome_path\n",
    "\n",
    "# Optional: uncomment this to run headless\n",
    "# options.add_argument(\"--headless\")\n",
    "\n",
    "# Automatically fetch correct driver\n",
    "service = Service(get_chromedriver_path())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# === Load Facebook and Inject Cookies ===\n",
    "driver.get(\"https://www.facebook.com/\")\n",
    "time.sleep(3)\n",
    "\n",
    "with open(COOKIES_PATH, \"r\") as f:\n",
    "    cookies = json.load(f)\n",
    "    for cookie in cookies:\n",
    "        if \"sameSite\" in cookie:\n",
    "            cookie[\"sameSite\"] = \"Strict\"\n",
    "        try:\n",
    "            driver.add_cookie(cookie)\n",
    "        except Exception as e:\n",
    "            print(\"cookie error:\", cookie.get(\"name\"), e)\n",
    "\n",
    "# === Navigate to Target Group ===\n",
    "driver.get(GROUP_URL)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What Did This Do?\n",
    "This section is your method to get into Facebook:\n",
    "- Without needing manual login, it uses saved cookies to impersonate a logged-in session.\n",
    "- This makes scraping faster, more consistent, and less likely to trigger Facebook’s security walls.\n",
    "- Once on the group page, the scraper can begin scanning for content like posts and timestamps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Scraper Function\n",
    "\n",
    "#### What It Does:\n",
    "- Automatically scrolls through the Facebook group page to load posts.\n",
    "- Extracts timestamp, post content, and post URL from each visible post.\n",
    "- Automatically stops if no new content is found after a couple scrolls.\n",
    "- Saves all scraped data into a clean, structured CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j4o's ultimate facebook group scraper v3.1 (cleaned on 4-8-2025)\n",
    "# features:\n",
    "# - relative timestamp conversion (3d → actual date)  (3-29-2025)\n",
    "# - error resilience against facebook's nonsense (3-29-2025)\n",
    "# - clean CSV output (3-30-2025)\n",
    "# - renamed for the group! (4-9-2025)\n",
    "\n",
    "# --------------------------\n",
    "# 1. SCROLLING MECHANISM\n",
    "# --------------------------\n",
    "def scroll_to_load(driver, max_scrolls=30, wait=2, min_scrolls=10, verbose=True):\n",
    "    \"\"\"scrolls the page with smart detection but ensures a minimum number of scrolls\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    stall_counter = 0\n",
    "\n",
    "    for i in range(max_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(wait)\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        posts_found = len(driver.find_elements(By.CSS_SELECTOR, \"div[role='article']\"))\n",
    "        if verbose:\n",
    "            print(f\"Scroll {i+1}/{max_scrolls} | Posts found: {posts_found}\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            stall_counter += 1\n",
    "            if i + 1 >= min_scrolls:\n",
    "                print(\"No new posts detected - stopping scroll\")\n",
    "                break\n",
    "        else:\n",
    "            stall_counter = 0\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "# --------------------------  \n",
    "# 2. TIMESTAMP CONVERSION\n",
    "# --------------------------\n",
    "def get_timestamp(post):\n",
    "    \"\"\"convert facebook's vague time hints into actual dates\"\"\"\n",
    "    time_elem = post.find(\"a\", href=lambda x: x and (\"/posts/\" in x or \"/permalink/\" in x))\n",
    "    if not time_elem:\n",
    "        return \"unknown\"\n",
    "\n",
    "    timestamp = time_elem.get_text(strip=True)\n",
    "\n",
    "    if \"Just now\" in timestamp:\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    match = re.match(r\"(\\d+)\\s*([smhdwy])\", timestamp)\n",
    "    if not match:\n",
    "        return timestamp\n",
    "\n",
    "    num, unit = int(match.group(1)), match.group(2)\n",
    "    now = datetime.now()\n",
    "\n",
    "    delta_map = {\n",
    "        's': timedelta(seconds=num),\n",
    "        'm': timedelta(minutes=num),\n",
    "        'h': timedelta(hours=num),\n",
    "        'd': timedelta(days=num),\n",
    "        'w': timedelta(weeks=num),\n",
    "        'y': timedelta(days=num * 365),\n",
    "    }\n",
    "\n",
    "    return (now - delta_map.get(unit, timedelta())).strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "# --------------------------\n",
    "# 3. MAIN EXTRACTION LOGIC\n",
    "# --------------------------\n",
    "def get_content(post):\n",
    "    \"\"\"extract post text with 'See more' expansion\"\"\"\n",
    "    content_div = post.find(\"div\", {\"dir\": \"auto\"})\n",
    "    if not content_div:\n",
    "        return \"\"\n",
    "\n",
    "    content = content_div.get_text(\"\\n\", strip=True)\n",
    "    see_more = post.find(\"div\", string=re.compile(r\"See\\\\s+more\", re.I))\n",
    "    if see_more:\n",
    "        more_text = see_more.find_next(\"div\")\n",
    "        if more_text:\n",
    "            content += \"\\n\" + more_text.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    return content\n",
    "\n",
    "def get_post_url(post):\n",
    "    \"\"\"extract post URL with fallback\"\"\"\n",
    "    time_elem = post.find(\"a\", href=lambda x: x and (\"/posts/\" in x or \"/permalink/\" in x))\n",
    "    if time_elem:\n",
    "        return urljoin(\"https://www.facebook.com\", time_elem['href'])\n",
    "    return \"unknown\"\n",
    "\n",
    "def extract_posts(driver):\n",
    "    \"\"\"More reliable post detection with Facebook's current layout\"\"\"\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    posts_data = []\n",
    "\n",
    "    post_selectors = [\n",
    "        {'role': 'article'},\n",
    "        {'data-pagelet': re.compile('FeedUnit_')},\n",
    "        {'class': re.compile('x1yztbdb')}\n",
    "    ]\n",
    "\n",
    "    for selector in post_selectors:\n",
    "        posts = soup.find_all(\"div\", selector)\n",
    "        if posts:\n",
    "            break\n",
    "\n",
    "    for post in posts:\n",
    "        try:\n",
    "            post_data = {\n",
    "                \"timestamp\": get_timestamp(post),\n",
    "                \"content\": get_content(post),\n",
    "                \"post_url\": get_post_url(post),\n",
    "            }\n",
    "            posts_data.append(post_data)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return posts_data\n",
    "\n",
    "# --------------------------\n",
    "# 4. OUTPUT FUNCTIONS\n",
    "# --------------------------\n",
    "def save_posts_to_csv(posts, filename=\"fb_scraped_data.csv\"):\n",
    "    \"\"\"save our hard-earned data to CSV\"\"\"\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"timestamp\", \"content\", \"post_url\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(posts)\n",
    "\n",
    "# --------------------------\n",
    "# 5. USER-FRIENDLY FILENAME HANDLING\n",
    "# --------------------------\n",
    "def get_output_filename():\n",
    "    while True:\n",
    "        user_input = input(\"Enter output filename (or press Enter for 'fb_scraped_data.csv'): \").strip()\n",
    "        if not user_input:\n",
    "            return \"fb_scraped_data.csv\"\n",
    "        if not user_input.lower().endswith('.csv'):\n",
    "            user_input += '.csv'\n",
    "        if len(user_input) > 100:\n",
    "            print(\"That filename is too long. Try something shorter.\")\n",
    "            continue\n",
    "        if not re.match(r'^[\\w\\-\\. ]+$', user_input):\n",
    "            print(\"Invalid characters in filename. Use only letters, numbers, spaces, hyphens, and periods.\")\n",
    "            continue\n",
    "        return user_input\n",
    "\n",
    "# --------------------------\n",
    "# 6. MAIN EXECUTION FLOW\n",
    "# --------------------------\n",
    "def run_scraper(driver):\n",
    "    try:\n",
    "        print(\"\\U0001f680 Starting Facebook scraper\")\n",
    "        scrolls_per_batch = 10\n",
    "\n",
    "        # === This is where you adjust the running time of the program ===\n",
    "        # Setting total run time to approximately 3 minutes\n",
    "        total_batches = 6  # 6 batches × ~30s each = ~3 minutes\n",
    "        wait = 3.0\n",
    "\n",
    "        all_posts = {}\n",
    "        stale_batches = 0\n",
    "\n",
    "        for batch in range(total_batches):\n",
    "            print(f\"Batch {batch+1}/{total_batches} | Scrolling {scrolls_per_batch} times...\")\n",
    "            scroll_to_load(driver, max_scrolls=scrolls_per_batch, wait=wait, verbose=False)\n",
    "\n",
    "            print(\"\\U0001f50d Extracting posts...\")\n",
    "            new_posts = extract_posts(driver)\n",
    "\n",
    "            new_count = 0\n",
    "            for p in new_posts:\n",
    "                if p[\"post_url\"] not in all_posts:\n",
    "                    all_posts[p[\"post_url\"]] = p\n",
    "                    new_count += 1\n",
    "\n",
    "            total_unique = len(all_posts)\n",
    "            print(f\"New: {new_count} | Total unique: {total_unique}\")\n",
    "\n",
    "            if new_count == 0:\n",
    "                stale_batches += 1\n",
    "                print(f\"\\u26a0\\ufe0f No new posts in this batch ({stale_batches}x)\")\n",
    "                if stale_batches >= 3:\n",
    "                    print(\"No new content in 3 batches. Stopping early.\")\n",
    "                    break\n",
    "            else:\n",
    "                stale_batches = 0\n",
    "\n",
    "        posts_list = list(all_posts.values())\n",
    "        print(f\"\\n✅ Finished scraping: {len(posts_list)} unique posts\")\n",
    "\n",
    "        if posts_list:\n",
    "            filename = get_output_filename()\n",
    "            save_posts_to_csv(posts_list, filename)\n",
    "            print(f\"Saved {len(posts_list)} posts to '{filename}'\")\n",
    "\n",
    "            print(\"\\nTop timestamps:\")\n",
    "            df = pd.DataFrame(posts_list)\n",
    "            print(df['timestamp'].value_counts().head(10))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Do I Make It Run Longer?\n",
    "\n",
    "Normally, the scraper runs for approximately 3 minutes, which is enough for testing or small-scale use.\n",
    "\n",
    "If you'd like to extend the scraping time:\n",
    "\n",
    "- Find this section in the `run_scraper` function:\n",
    "\n",
    "  ```python\n",
    "  # === This is where you adjust the running time of the program ===\n",
    "  total_batches = 6  # 6 batches × ~30s each = ~3 minutes\n",
    "    ```\n",
    "    \n",
    "#### Increase `total_batches` like this:\n",
    "- `12` → ~6 minutes  \n",
    "- `20` → ~10 minutes  \n",
    "- `30` → ~15 minutes+\n",
    "\n",
    "Each batch:\n",
    "- Scrolls the page 10 times (`scrolls_per_batch = 10`)\n",
    "- Waits ~2.5 seconds between scrolls\n",
    "\n",
    "> Longer runs **WILL** trigger Facebook's security measures and you may be rate-limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run The Scraper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scraper(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running will save to your experiments folder, so you can just run the CSV below by inputting the filename."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
